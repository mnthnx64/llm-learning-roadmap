{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LangChain Framework\n",
        "\n",
        "This notebook covers the LangChain framework - a powerful library for building applications with Large Language Models:\n",
        "- **What is LangChain?**: A framework for developing LLM-powered applications\n",
        "- **Core Components**: Models, Prompts, Chains, Agents, and Memory\n",
        "- **Document Processing**: Loaders, text splitters, and vector stores\n",
        "- **Chains**: Combining LLMs with other components\n",
        "- **Agents**: Building autonomous systems that use tools\n",
        "- **Memory**: Managing conversation history and context\n",
        "- **Practical Applications**: Building real-world LLM applications\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "- Understand what LangChain is and why it's useful\n",
        "- Learn the core components of LangChain\n",
        "- Work with document loaders and text splitters\n",
        "- Build chains to combine LLM calls\n",
        "- Create agents that can use tools\n",
        "- Implement memory for conversational applications\n",
        "- Build end-to-end LangChain applications\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation\n",
        "\n",
        "Run this cell to install required packages (uncomment if needed):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install packages (uncomment if needed)\n",
        "# !pip install langchain langchain-openai langchain-community langchain-core chromadb sentence-transformers pypdf python-dotenv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. What is LangChain?\n",
        "\n",
        "**LangChain** is an open-source framework for building applications powered by Large Language Models (LLMs). It provides:\n",
        "\n",
        "### Key Features:\n",
        "\n",
        "1. **Modularity**: Break down complex LLM applications into reusable components\n",
        "2. **Chains**: Combine multiple components (LLMs, prompts, tools) into sequences\n",
        "3. **Agents**: Build autonomous systems that can use tools and make decisions\n",
        "4. **Memory**: Manage conversation history and context\n",
        "5. **Data Integration**: Connect to various data sources (documents, databases, APIs)\n",
        "6. **Vector Stores**: Integrate with vector databases for RAG applications\n",
        "\n",
        "### Why LangChain?\n",
        "\n",
        "- **Abstraction**: Simplifies working with different LLM providers (OpenAI, Anthropic, etc.)\n",
        "- **Composability**: Build complex applications from simple building blocks\n",
        "- **Production-Ready**: Includes features like streaming, callbacks, and observability\n",
        "- **Ecosystem**: Large community and extensive integrations\n",
        "\n",
        "### LangChain Architecture:\n",
        "\n",
        "```\n",
        "┌─────────────┐\n",
        "│   Models    │  ← LLMs, Chat Models, Embeddings\n",
        "└─────────────┘\n",
        "       ↓\n",
        "┌─────────────┐\n",
        "│   Prompts   │  ← Prompt templates, output parsers\n",
        "└─────────────┘\n",
        "       ↓\n",
        "┌─────────────┐\n",
        "│   Chains    │  ← Combine models, prompts, tools\n",
        "└─────────────┘\n",
        "       ↓\n",
        "┌─────────────┐\n",
        "│   Agents    │  ← Autonomous systems with tools\n",
        "└─────────────┘\n",
        "       ↓\n",
        "┌─────────────┐\n",
        "│   Memory    │  ← Conversation history\n",
        "└─────────────┘\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Core Components Overview\n",
        "\n",
        "LangChain consists of several core components:\n",
        "\n",
        "1. **Models**: Interface with different LLM providers\n",
        "2. **Prompts**: Template and manage prompts\n",
        "3. **Chains**: Combine multiple components\n",
        "4. **Agents**: Autonomous systems that use tools\n",
        "5. **Memory**: Store and retrieve conversation history\n",
        "6. **Document Loaders**: Load data from various sources\n",
        "7. **Text Splitters**: Split documents into chunks\n",
        "8. **Vector Stores**: Store and retrieve embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ LangChain libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import core LangChain libraries\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Core LangChain imports\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.chains import LLMChain, SimpleSequentialChain, SequentialChain\n",
        "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory\n",
        "from langchain.agents import initialize_agent, AgentType, create_react_agent\n",
        "from langchain.tools import Tool\n",
        "\n",
        "# Document processing\n",
        "from langchain_community.document_loaders import TextLoader, PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
        "\n",
        "# Vector stores\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "print(\"✅ LangChain libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Models\n",
        "\n",
        "LangChain provides a unified interface for working with different LLM providers. The main model types are:\n",
        "\n",
        "- **LLMs**: Text-in, text-out models (e.g., GPT-3)\n",
        "- **Chat Models**: Message-in, message-out models (e.g., GPT-4, Claude)\n",
        "- **Embeddings**: Convert text to vectors\n",
        "\n",
        "### 3.1 Chat Models (Recommended)\n",
        "\n",
        "Chat models use a message-based interface with system, human, and AI messages.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize a Chat Model (OpenAI)\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "if api_key:\n",
        "    # Create a ChatOpenAI instance\n",
        "    llm = ChatOpenAI(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        temperature=0.7,  # Controls randomness (0.0 = deterministic, 1.0 = creative)\n",
        "        openai_api_key=api_key\n",
        "    )\n",
        "    \n",
        "    # Simple invocation\n",
        "    response = llm.invoke(\"What is machine learning in one sentence?\")\n",
        "    print(\"Response:\", response.content)\n",
        "else:\n",
        "    print(\"⚠️  OPENAI_API_KEY not found in environment variables.\")\n",
        "    print(\"Please set your OpenAI API key to use the LLM.\")\n",
        "    print(\"You can use: export OPENAI_API_KEY='your-key-here'\")\n",
        "    llm = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Using Messages\n",
        "\n",
        "Chat models work with message objects for structured conversations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if llm:\n",
        "    from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
        "    \n",
        "    # Create messages\n",
        "    messages = [\n",
        "        SystemMessage(content=\"You are a helpful AI assistant that explains concepts simply.\"),\n",
        "        HumanMessage(content=\"Explain what a neural network is in 2 sentences.\")\n",
        "    ]\n",
        "    \n",
        "    # Invoke with messages\n",
        "    response = llm.invoke(messages)\n",
        "    print(\"Response:\", response.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Prompts\n",
        "\n",
        "Prompts are templates that help structure inputs to LLMs. LangChain provides:\n",
        "\n",
        "- **Prompt Templates**: Reusable prompt structures with variables\n",
        "- **Chat Prompt Templates**: For chat models with system/human/AI messages\n",
        "- **Output Parsers**: Structure and validate LLM outputs\n",
        "\n",
        "### 4.1 Prompt Templates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a prompt template\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\", \"audience\"],\n",
        "    template=\"Explain {topic} to a {audience} in simple terms. Keep it under 100 words.\"\n",
        ")\n",
        "\n",
        "# Format the prompt\n",
        "formatted_prompt = prompt.format(topic=\"quantum computing\", audience=\"10-year-old\")\n",
        "print(\"Formatted Prompt:\")\n",
        "print(formatted_prompt)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# Use with LLM\n",
        "if llm:\n",
        "    # Note: ChatOpenAI expects messages, so we'll use ChatPromptTemplate instead\n",
        "    # But PromptTemplate works with text-in/text-out LLMs\n",
        "    print(\"Note: For ChatOpenAI, use ChatPromptTemplate (shown next)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Chat Prompt Templates\n",
        "\n",
        "Chat prompt templates are designed for chat models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a chat prompt template\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful AI tutor that explains {subject} concepts.\"),\n",
        "    (\"human\", \"Explain {topic} in simple terms with an example.\")\n",
        "])\n",
        "\n",
        "# Format with variables\n",
        "messages = chat_prompt.format_messages(\n",
        "    subject=\"computer science\",\n",
        "    topic=\"recursion\"\n",
        ")\n",
        "\n",
        "print(\"Formatted Messages:\")\n",
        "for msg in messages:\n",
        "    print(f\"{msg.__class__.__name__}: {msg.content}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# Use with LLM\n",
        "if llm:\n",
        "    response = llm.invoke(messages)\n",
        "    print(\"LLM Response:\")\n",
        "    print(response.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Output Parsers\n",
        "\n",
        "Output parsers help structure and validate LLM responses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple string output parser\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "if llm:\n",
        "    # Create a chain: prompt -> LLM -> parser\n",
        "    chain = chat_prompt | llm | output_parser\n",
        "    \n",
        "    # Invoke the chain\n",
        "    result = chain.invoke({\n",
        "        \"subject\": \"mathematics\",\n",
        "        \"topic\": \"Pythagorean theorem\"\n",
        "    })\n",
        "    \n",
        "    print(\"Parsed Output:\")\n",
        "    print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Chains\n",
        "\n",
        "**Chains** are sequences of calls to LLMs and other utilities. They allow you to combine multiple components:\n",
        "\n",
        "- **LLMChain**: Basic chain with a prompt and LLM\n",
        "- **Sequential Chains**: Chain multiple LLM calls together\n",
        "- **Router Chains**: Route inputs to different chains\n",
        "- **LCEL (LangChain Expression Language)**: Modern way to compose chains using `|` operator\n",
        "\n",
        "### 5.1 LLMChain\n",
        "\n",
        "The simplest chain that combines a prompt template with an LLM.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if llm:\n",
        "    # Create a prompt template\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\"human\", \"Write a {style} summary of: {text}\")\n",
        "    ])\n",
        "    \n",
        "    # Create an LLMChain (using LCEL syntax)\n",
        "    chain = prompt | llm | output_parser\n",
        "    \n",
        "    # Run the chain\n",
        "    result = chain.invoke({\n",
        "        \"style\": \"concise\",\n",
        "        \"text\": \"Machine learning is a subset of artificial intelligence that enables systems to learn from data.\"\n",
        "    })\n",
        "    \n",
        "    print(\"Chain Result:\")\n",
        "    print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Sequential Chains\n",
        "\n",
        "Sequential chains allow you to chain multiple LLM calls where the output of one becomes the input of the next.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if llm:\n",
        "    # Step 1: Generate a story\n",
        "    story_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"You are a creative writer.\"),\n",
        "        (\"human\", \"Write a short 2-sentence story about {topic}\")\n",
        "    ])\n",
        "    story_chain = story_prompt | llm | output_parser\n",
        "    \n",
        "    # Step 2: Translate the story\n",
        "    translate_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"You are a translator.\"),\n",
        "        (\"human\", \"Translate the following text to {language}:\\n\\n{text}\")\n",
        "    ])\n",
        "    translate_chain = translate_prompt | llm | output_parser\n",
        "    \n",
        "    # Run sequentially\n",
        "    topic = \"a robot learning to paint\"\n",
        "    story = story_chain.invoke({\"topic\": topic})\n",
        "    print(\"Original Story:\")\n",
        "    print(story)\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "    \n",
        "    translation = translate_chain.invoke({\"language\": \"Spanish\", \"text\": story})\n",
        "    print(\"Translated Story:\")\n",
        "    print(translation)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3 LCEL (LangChain Expression Language)\n",
        "\n",
        "LCEL is the modern way to compose chains using the `|` operator. It's more flexible and supports streaming, parallelization, and more.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if llm:\n",
        "    # Create a complex chain using LCEL\n",
        "    prompt1 = ChatPromptTemplate.from_messages([\n",
        "        (\"human\", \"List 3 key points about {topic}\")\n",
        "    ])\n",
        "    \n",
        "    prompt2 = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"You are a teacher explaining concepts to students.\"),\n",
        "        (\"human\", \"Explain these points in detail:\\n{points}\")\n",
        "    ])\n",
        "    \n",
        "    # Compose chains\n",
        "    chain = (\n",
        "        prompt1 | llm | output_parser\n",
        "    ).pipe(\n",
        "        lambda x: {\"points\": x}\n",
        "    ).pipe(\n",
        "        prompt2 | llm | output_parser\n",
        "    )\n",
        "    \n",
        "    result = chain.invoke({\"topic\": \"neural networks\"})\n",
        "    print(\"Chain Result:\")\n",
        "    print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Document Loaders\n",
        "\n",
        "Document loaders help you load data from various sources (files, web pages, databases, etc.) into LangChain Document objects.\n",
        "\n",
        "### 6.1 Text File Loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a sample text file for demonstration\n",
        "sample_text = \"\"\"\n",
        "Machine Learning Fundamentals\n",
        "\n",
        "Machine learning is a subset of artificial intelligence that focuses on the development of algorithms \n",
        "and statistical models that enable computer systems to improve their performance on a specific task \n",
        "through experience, without being explicitly programmed.\n",
        "\n",
        "Key Concepts:\n",
        "1. Supervised Learning: Learning from labeled data\n",
        "2. Unsupervised Learning: Finding patterns in unlabeled data\n",
        "3. Reinforcement Learning: Learning through interaction and rewards\n",
        "\n",
        "Applications include image recognition, natural language processing, recommendation systems, and more.\n",
        "\"\"\"\n",
        "\n",
        "# Write to a temporary file\n",
        "with open(\"sample_ml.txt\", \"w\") as f:\n",
        "    f.write(sample_text)\n",
        "\n",
        "# Load the document\n",
        "try:\n",
        "    loader = TextLoader(\"sample_ml.txt\")\n",
        "    documents = loader.load()\n",
        "    \n",
        "    print(f\"Loaded {len(documents)} document(s)\")\n",
        "    print(f\"Document content (first 200 chars): {documents[0].page_content[:200]}...\")\n",
        "    print(f\"\\nMetadata: {documents[0].metadata}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading document: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 PDF Loader\n",
        "\n",
        "PDF loaders can extract text from PDF files. (Note: Requires pypdf package)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example of PDF loading (commented out - requires a PDF file)\n",
        "# loader = PyPDFLoader(\"path/to/document.pdf\")\n",
        "# documents = loader.load()\n",
        "\n",
        "print(\"PDF loading example:\")\n",
        "print(\"To load a PDF, use: PyPDFLoader('path/to/file.pdf')\")\n",
        "print(\"This requires the pypdf package: pip install pypdf\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Text Splitters\n",
        "\n",
        "Text splitters break documents into smaller chunks, which is essential for:\n",
        "- **Vector Storage**: Embeddings work better with smaller chunks\n",
        "- **Context Windows**: LLMs have token limits\n",
        "- **Retrieval**: Smaller chunks improve retrieval precision\n",
        "\n",
        "### 7.1 Recursive Character Text Splitter\n",
        "\n",
        "The most commonly used splitter that tries to split on different separators recursively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a text splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=100,      # Maximum size of chunks (in characters)\n",
        "    chunk_overlap=20,    # Overlap between chunks to preserve context\n",
        "    length_function=len, # Function to measure length\n",
        ")\n",
        "\n",
        "# Split the document we loaded earlier\n",
        "if 'documents' in locals() and documents:\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "    \n",
        "    print(f\"Split into {len(chunks)} chunks\")\n",
        "    print(f\"\\nChunk sizes: {[len(chunk.page_content) for chunk in chunks]}\")\n",
        "    print(f\"\\nFirst chunk:\\n{chunks[0].page_content}\")\n",
        "    print(f\"\\nSecond chunk:\\n{chunks[1].page_content}\")\n",
        "else:\n",
        "    # Use sample text\n",
        "    sample_doc = sample_text\n",
        "    chunks = text_splitter.split_text(sample_doc)\n",
        "    print(f\"Split into {len(chunks)} chunks\")\n",
        "    for i, chunk in enumerate(chunks[:3], 1):\n",
        "        print(f\"\\nChunk {i} ({len(chunk)} chars):\\n{chunk[:100]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Vector Stores and RAG\n",
        "\n",
        "Vector stores allow you to store and retrieve document embeddings. This is the foundation of RAG (Retrieval Augmented Generation).\n",
        "\n",
        "### 8.1 Creating a Vector Store with Chroma\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create embeddings and vector store\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "if api_key:\n",
        "    # Initialize embeddings\n",
        "    embeddings = OpenAIEmbeddings(openai_api_key=api_key)\n",
        "    \n",
        "    # Create sample documents\n",
        "    sample_docs = [\n",
        "        \"Machine learning is a subset of AI that learns from data.\",\n",
        "        \"Deep learning uses neural networks with multiple layers.\",\n",
        "        \"Natural language processing helps computers understand human language.\",\n",
        "        \"Computer vision enables machines to interpret visual information.\",\n",
        "        \"Reinforcement learning learns through interaction and rewards.\"\n",
        "    ]\n",
        "    \n",
        "    # Create vector store\n",
        "    vectorstore = Chroma.from_texts(\n",
        "        texts=sample_docs,\n",
        "        embedding=embeddings,\n",
        "        persist_directory=\"./langchain_chroma_db\"\n",
        "    )\n",
        "    \n",
        "    print(\"✅ Vector store created successfully!\")\n",
        "    print(f\"Stored {len(sample_docs)} documents\")\n",
        "else:\n",
        "    print(\"⚠️  OPENAI_API_KEY not found. Skipping vector store creation.\")\n",
        "    vectorstore = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.2 Retrieval and RAG Chain\n",
        "\n",
        "Combine vector store retrieval with LLM generation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if vectorstore and llm:\n",
        "    from langchain.chains import RetrievalQA\n",
        "    \n",
        "    # Create a retrieval chain\n",
        "    retrieval_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",  # \"stuff\", \"map_reduce\", \"refine\", \"map_rerank\"\n",
        "        retriever=vectorstore.as_retriever(search_kwargs={\"k\": 2}),\n",
        "        return_source_documents=True\n",
        "    )\n",
        "    \n",
        "    # Query the chain\n",
        "    query = \"What is machine learning?\"\n",
        "    result = retrieval_chain.invoke({\"query\": query})\n",
        "    \n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"\\nAnswer: {result['result']}\")\n",
        "    print(f\"\\nSource documents used: {len(result['source_documents'])}\")\n",
        "    for i, doc in enumerate(result['source_documents'], 1):\n",
        "        print(f\"\\nSource {i}: {doc.page_content[:100]}...\")\n",
        "else:\n",
        "    print(\"⚠️  Vector store or LLM not available. Skipping RAG example.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.3 Using LCEL for RAG\n",
        "\n",
        "Modern way to build RAG chains using LCEL.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if vectorstore and llm:\n",
        "    from langchain_core.runnables import RunnablePassthrough\n",
        "    \n",
        "    # Create a retriever\n",
        "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
        "    \n",
        "    # Create prompt template\n",
        "    rag_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"You are a helpful assistant. Use the following context to answer questions.\\n\\nContext: {context}\"),\n",
        "        (\"human\", \"{question}\")\n",
        "    ])\n",
        "    \n",
        "    # Format documents function\n",
        "    def format_docs(docs):\n",
        "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "    \n",
        "    # Create RAG chain using LCEL\n",
        "    rag_chain = (\n",
        "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "        | rag_prompt\n",
        "        | llm\n",
        "        | output_parser\n",
        "    )\n",
        "    \n",
        "    # Query\n",
        "    result = rag_chain.invoke(\"What is deep learning?\")\n",
        "    print(\"RAG Chain Result:\")\n",
        "    print(result)\n",
        "else:\n",
        "    print(\"⚠️  Vector store or LLM not available. Skipping LCEL RAG example.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Agents\n",
        "\n",
        "**Agents** are autonomous systems that can use tools, make decisions, and take actions. They use an LLM to decide which actions to take.\n",
        "\n",
        "### Key Concepts:\n",
        "\n",
        "- **Tools**: Functions that agents can call (e.g., search, calculator, API calls)\n",
        "- **Agent Types**: Different reasoning strategies (ReAct, Plan-and-Execute, etc.)\n",
        "- **Agent Executor**: Runs the agent loop\n",
        "\n",
        "### 9.1 Creating Simple Tools\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define custom tools\n",
        "def calculate(expression: str) -> str:\n",
        "    \"\"\"Evaluates a mathematical expression safely.\"\"\"\n",
        "    try:\n",
        "        # Simple calculator - in production, use a safer evaluation method\n",
        "        result = eval(expression)\n",
        "        return str(result)\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "def get_word_length(word: str) -> str:\n",
        "    \"\"\"Returns the length of a word.\"\"\"\n",
        "    return str(len(word))\n",
        "\n",
        "# Create Tool objects\n",
        "calculator_tool = Tool(\n",
        "    name=\"Calculator\",\n",
        "    func=calculate,\n",
        "    description=\"Useful for performing mathematical calculations. Input should be a valid Python expression.\"\n",
        ")\n",
        "\n",
        "word_length_tool = Tool(\n",
        "    name=\"WordLength\",\n",
        "    func=get_word_length,\n",
        "    description=\"Useful for getting the length of a word. Input should be a single word.\"\n",
        ")\n",
        "\n",
        "tools = [calculator_tool, word_length_tool]\n",
        "\n",
        "print(\"✅ Tools created:\")\n",
        "for tool in tools:\n",
        "    print(f\"  - {tool.name}: {tool.description}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.2 Creating an Agent with Tools\n",
        "\n",
        "Agents can use tools to perform actions and answer questions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if llm:\n",
        "    # Create an agent using the ReAct framework\n",
        "    # ReAct: Reasoning + Acting - the agent reasons about what to do, then acts\n",
        "    \n",
        "    from langchain.agents import AgentExecutor, create_react_agent\n",
        "    from langchain import hub\n",
        "    \n",
        "    # Get the ReAct prompt template\n",
        "    try:\n",
        "        prompt = hub.pull(\"hwchase17/react\")\n",
        "    except:\n",
        "        # Fallback prompt if hub is not available\n",
        "        from langchain_core.prompts import PromptTemplate\n",
        "        prompt = PromptTemplate.from_template(\"\"\"\n",
        "You are a helpful assistant that can use tools to answer questions.\n",
        "\n",
        "You have access to the following tools:\n",
        "{tools}\n",
        "\n",
        "Use the following format:\n",
        "\n",
        "Question: the input question you must answer\n",
        "Thought: you should always think about what to do\n",
        "Action: the action to take, should be one of [{tool_names}]\n",
        "Action Input: the input to the action\n",
        "Observation: the result of the action\n",
        "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
        "Thought: I now know the final answer\n",
        "Final Answer: the final answer to the original input question\n",
        "\n",
        "Begin!\n",
        "\n",
        "Question: {input}\n",
        "Thought: {agent_scratchpad}\n",
        "\"\"\")\n",
        "    \n",
        "    # Create the agent\n",
        "    agent = create_react_agent(llm, tools, prompt)\n",
        "    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "    \n",
        "    # Test the agent\n",
        "    print(\"Testing agent with calculation:\")\n",
        "    result = agent_executor.invoke({\"input\": \"What is 15 * 8 + 23?\"})\n",
        "    print(f\"\\nFinal Answer: {result['output']}\")\n",
        "else:\n",
        "    print(\"⚠️  LLM not available. Skipping agent example.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.3 Agent with Vector Store Tool\n",
        "\n",
        "Agents can use vector stores as tools for retrieval.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if vectorstore and llm:\n",
        "    from langchain.tools.retriever import create_retriever_tool\n",
        "    \n",
        "    # Create a retriever tool\n",
        "    retriever_tool = create_retriever_tool(\n",
        "        vectorstore.as_retriever(),\n",
        "        \"knowledge_base\",\n",
        "        \"Searches and returns information about machine learning, AI, and related topics.\"\n",
        "    )\n",
        "    \n",
        "    # Combine tools\n",
        "    all_tools = [retriever_tool, calculator_tool]\n",
        "    \n",
        "    # Create agent with retrieval capability\n",
        "    try:\n",
        "        prompt = hub.pull(\"hwchase17/react\")\n",
        "    except:\n",
        "        from langchain_core.prompts import PromptTemplate\n",
        "        prompt = PromptTemplate.from_template(\"\"\"\n",
        "You are a helpful assistant with access to a knowledge base and calculator.\n",
        "\n",
        "Tools: {tools}\n",
        "Tool Names: {tool_names}\n",
        "\n",
        "Use the format:\n",
        "Question: {input}\n",
        "Thought: {agent_scratchpad}\n",
        "\"\"\")\n",
        "    \n",
        "    agent = create_react_agent(llm, all_tools, prompt)\n",
        "    agent_executor = AgentExecutor(agent=agent, tools=all_tools, verbose=True)\n",
        "    \n",
        "    # Test with a question that requires retrieval\n",
        "    print(\"Testing agent with knowledge retrieval:\")\n",
        "    result = agent_executor.invoke({\"input\": \"What is machine learning? Also calculate 10 * 5.\"})\n",
        "    print(f\"\\nFinal Answer: {result['output']}\")\n",
        "else:\n",
        "    print(\"⚠️  Vector store or LLM not available. Skipping retrieval agent example.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Memory\n",
        "\n",
        "**Memory** allows chains and agents to remember information from previous interactions. This is essential for conversational applications.\n",
        "\n",
        "### Types of Memory:\n",
        "\n",
        "- **ConversationBufferMemory**: Stores all conversation history\n",
        "- **ConversationBufferWindowMemory**: Stores only the last N messages\n",
        "- **ConversationSummaryMemory**: Summarizes conversation history\n",
        "- **ConversationSummaryBufferMemory**: Combines summary and buffer\n",
        "\n",
        "### 10.1 Conversation Buffer Memory\n",
        "\n",
        "Stores the entire conversation history.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if llm:\n",
        "    from langchain.chains import ConversationChain\n",
        "    \n",
        "    # Create memory\n",
        "    memory = ConversationBufferMemory()\n",
        "    \n",
        "    # Create a conversation chain\n",
        "    conversation = ConversationChain(\n",
        "        llm=llm,\n",
        "        memory=memory,\n",
        "        verbose=True\n",
        "    )\n",
        "    \n",
        "    # First interaction\n",
        "    print(\"First interaction:\")\n",
        "    response1 = conversation.predict(input=\"Hi, my name is Alice. I love machine learning.\")\n",
        "    print(f\"AI: {response1}\\n\")\n",
        "    \n",
        "    # Second interaction (should remember the name)\n",
        "    print(\"Second interaction:\")\n",
        "    response2 = conversation.predict(input=\"What's my name and what do I love?\")\n",
        "    print(f\"AI: {response2}\\n\")\n",
        "    \n",
        "    # Check memory\n",
        "    print(\"Memory contents:\")\n",
        "    print(memory.buffer)\n",
        "else:\n",
        "    print(\"⚠️  LLM not available. Skipping memory example.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10.2 Conversation Buffer Window Memory\n",
        "\n",
        "Stores only the last N messages to limit memory usage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if llm:\n",
        "    # Create window memory (keeps last 2 exchanges)\n",
        "    window_memory = ConversationBufferWindowMemory(k=2)\n",
        "    \n",
        "    conversation = ConversationChain(\n",
        "        llm=llm,\n",
        "        memory=window_memory,\n",
        "        verbose=False\n",
        "    )\n",
        "    \n",
        "    # Multiple interactions\n",
        "    conversation.predict(input=\"I'm learning Python programming.\")\n",
        "    conversation.predict(input=\"I also like data science.\")\n",
        "    conversation.predict(input=\"What programming language am I learning?\")\n",
        "    \n",
        "    print(\"Window Memory (last 2 exchanges):\")\n",
        "    print(window_memory.buffer)\n",
        "else:\n",
        "    print(\"⚠️  LLM not available. Skipping window memory example.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10.3 Memory with LCEL Chains\n",
        "\n",
        "Using memory with modern LCEL chains.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if llm:\n",
        "    from langchain_core.chat_history import InMemoryChatMessageHistory\n",
        "    from langchain_core.chat_messages import HumanMessage, AIMessage\n",
        "    from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "    \n",
        "    # Create a simple chain\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"You are a helpful assistant. Remember information from the conversation.\"),\n",
        "        (\"placeholder\", \"{messages}\"),\n",
        "        (\"human\", \"{input}\")\n",
        "    ])\n",
        "    \n",
        "    chain = prompt | llm | output_parser\n",
        "    \n",
        "    # Create message history store\n",
        "    store = {}\n",
        "    \n",
        "    def get_session_history(session_id: str):\n",
        "        if session_id not in store:\n",
        "            store[session_id] = InMemoryChatMessageHistory()\n",
        "        return store[session_id]\n",
        "    \n",
        "    # Add message history\n",
        "    chain_with_history = RunnableWithMessageHistory(\n",
        "        chain,\n",
        "        get_session_history,\n",
        "        input_messages_key=\"input\",\n",
        "        history_messages_key=\"messages\"\n",
        "    )\n",
        "    \n",
        "    # Use the chain with history\n",
        "    config = {\"configurable\": {\"session_id\": \"conversation_1\"}}\n",
        "    \n",
        "    print(\"First message:\")\n",
        "    result1 = chain_with_history.invoke(\n",
        "        {\"input\": \"My favorite color is blue.\"},\n",
        "        config=config\n",
        "    )\n",
        "    print(f\"AI: {result1}\\n\")\n",
        "    \n",
        "    print(\"Second message (should remember):\")\n",
        "    result2 = chain_with_history.invoke(\n",
        "        {\"input\": \"What's my favorite color?\"},\n",
        "        config=config\n",
        "    )\n",
        "    print(f\"AI: {result2}\")\n",
        "else:\n",
        "    print(\"⚠️  LLM not available. Skipping LCEL memory example.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Practical Examples\n",
        "\n",
        "Let's build some practical applications combining multiple LangChain components.\n",
        "\n",
        "### 11.1 Document Q&A System\n",
        "\n",
        "A complete system that loads documents, creates embeddings, and answers questions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if llm and api_key:\n",
        "    # Step 1: Load and split documents\n",
        "    if 'documents' in locals() and documents:\n",
        "        # Use existing documents\n",
        "        doc_chunks = text_splitter.split_documents(documents)\n",
        "    else:\n",
        "        # Create sample documents\n",
        "        sample_docs = [\n",
        "            \"LangChain is a framework for building LLM applications. It provides modular components.\",\n",
        "            \"Chains allow you to combine multiple LLM calls and tools in sequence.\",\n",
        "            \"Agents can autonomously use tools to complete tasks.\",\n",
        "            \"Vector stores enable semantic search over documents using embeddings.\"\n",
        "        ]\n",
        "        doc_chunks = [{\"page_content\": doc} for doc in sample_docs]\n",
        "    \n",
        "    # Step 2: Create vector store\n",
        "    embeddings = OpenAIEmbeddings(openai_api_key=api_key)\n",
        "    if isinstance(doc_chunks[0], dict):\n",
        "        texts = [doc[\"page_content\"] for doc in doc_chunks]\n",
        "        vectorstore = Chroma.from_texts(texts, embeddings)\n",
        "    else:\n",
        "        vectorstore = Chroma.from_documents(doc_chunks, embeddings)\n",
        "    \n",
        "    # Step 3: Create RAG chain\n",
        "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
        "    \n",
        "    rag_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"Answer the question based on the context. If you don't know, say so.\\n\\nContext: {context}\"),\n",
        "        (\"human\", \"{question}\")\n",
        "    ])\n",
        "    \n",
        "    def format_docs(docs):\n",
        "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "    \n",
        "    qa_chain = (\n",
        "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "        | rag_prompt\n",
        "        | llm\n",
        "        | output_parser\n",
        "    )\n",
        "    \n",
        "    # Step 4: Ask questions\n",
        "    questions = [\n",
        "        \"What is LangChain?\",\n",
        "        \"What are chains used for?\"\n",
        "    ]\n",
        "    \n",
        "    for question in questions:\n",
        "        print(f\"Q: {question}\")\n",
        "        answer = qa_chain.invoke(question)\n",
        "        print(f\"A: {answer}\\n\")\n",
        "else:\n",
        "    print(\"⚠️  LLM or API key not available. Skipping Q&A system example.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 11.2 Conversational Agent with Tools\n",
        "\n",
        "An agent that can have conversations and use tools.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if llm:\n",
        "    # Create tools\n",
        "    tools = [calculator_tool, word_length_tool]\n",
        "    \n",
        "    # Create agent with memory\n",
        "    try:\n",
        "        prompt = hub.pull(\"hwchase17/react-chat\")\n",
        "    except:\n",
        "        from langchain_core.prompts import MessagesPlaceholder\n",
        "        prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"You are a helpful assistant with access to tools.\"),\n",
        "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "            (\"human\", \"{input}\"),\n",
        "            MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
        "        ])\n",
        "    \n",
        "    agent = create_react_agent(llm, tools, prompt)\n",
        "    \n",
        "    # Add memory\n",
        "    from langchain.agents import AgentExecutor\n",
        "    from langchain.memory import ConversationBufferMemory\n",
        "    \n",
        "    memory = ConversationBufferMemory(\n",
        "        memory_key=\"chat_history\",\n",
        "        return_messages=True\n",
        "    )\n",
        "    \n",
        "    agent_executor = AgentExecutor(\n",
        "        agent=agent,\n",
        "        tools=tools,\n",
        "        memory=memory,\n",
        "        verbose=True\n",
        "    )\n",
        "    \n",
        "    # Test the conversational agent\n",
        "    print(\"Conversation 1:\")\n",
        "    result1 = agent_executor.invoke({\"input\": \"Hi, I'm Bob. Calculate 25 * 4 for me.\"})\n",
        "    print(f\"Response: {result1['output']}\\n\")\n",
        "    \n",
        "    print(\"Conversation 2 (should remember name):\")\n",
        "    result2 = agent_executor.invoke({\"input\": \"What's my name and what was the calculation result?\"})\n",
        "    print(f\"Response: {result2['output']}\")\n",
        "else:\n",
        "    print(\"⚠️  LLM not available. Skipping conversational agent example.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Best Practices and Tips\n",
        "\n",
        "### 12.1 Error Handling\n",
        "\n",
        "Always handle errors gracefully in production applications.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if llm:\n",
        "    # Example: Safe chain invocation with error handling\n",
        "    def safe_invoke(chain, input_data):\n",
        "        try:\n",
        "            result = chain.invoke(input_data)\n",
        "            return {\"success\": True, \"result\": result}\n",
        "        except Exception as e:\n",
        "            return {\"success\": False, \"error\": str(e)}\n",
        "    \n",
        "    # Test with valid input\n",
        "    chain = ChatPromptTemplate.from_messages([\n",
        "        (\"human\", \"{question}\")\n",
        "    ]) | llm | output_parser\n",
        "    \n",
        "    result = safe_invoke(chain, {\"question\": \"What is AI?\"})\n",
        "    print(\"Result:\", result)\n",
        "else:\n",
        "    print(\"⚠️  LLM not available. Skipping error handling example.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 12.2 Streaming Responses\n",
        "\n",
        "Stream responses for better user experience.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if llm:\n",
        "    # Create a chain\n",
        "    chain = ChatPromptTemplate.from_messages([\n",
        "        (\"human\", \"Write a short story about {topic}\")\n",
        "    ]) | llm\n",
        "    \n",
        "    # Stream the response\n",
        "    print(\"Streaming response:\")\n",
        "    print(\"-\" * 50)\n",
        "    for chunk in chain.stream({\"topic\": \"a robot learning to code\"}):\n",
        "        if hasattr(chunk, 'content'):\n",
        "            print(chunk.content, end=\"\", flush=True)\n",
        "    print(\"\\n\" + \"-\" * 50)\n",
        "else:\n",
        "    print(\"⚠️  LLM not available. Skipping streaming example.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 12.3 Key Takeaways\n",
        "\n",
        "1. **Use LCEL**: Prefer LCEL (`|` operator) for building chains - it's more flexible and modern\n",
        "2. **Chunk Documents**: Always split large documents into smaller chunks for better retrieval\n",
        "3. **Manage Memory**: Use appropriate memory types based on your use case (buffer vs window vs summary)\n",
        "4. **Error Handling**: Always wrap LLM calls in try-except blocks\n",
        "5. **Streaming**: Use streaming for better UX in production applications\n",
        "6. **Tool Descriptions**: Write clear tool descriptions so agents know when to use them\n",
        "7. **Temperature**: Adjust temperature based on task (lower for factual, higher for creative)\n",
        "8. **Vector Stores**: Choose the right vector store for your scale (Chroma for small, Pinecone/Weaviate for large)\n",
        "\n",
        "## 13. Next Steps\n",
        "\n",
        "Now that you understand LangChain fundamentals, you can:\n",
        "\n",
        "1. **Build RAG Applications**: Combine document loaders, vector stores, and chains\n",
        "2. **Create Agents**: Build autonomous systems with tools and memory\n",
        "3. **Explore Advanced Features**: \n",
        "   - LangGraph for complex agent workflows\n",
        "   - LangSmith for observability and debugging\n",
        "   - Custom tools and integrations\n",
        "4. **Production Deployment**: \n",
        "   - Add error handling and retries\n",
        "   - Implement caching\n",
        "   - Set up monitoring and logging\n",
        "\n",
        "## Resources\n",
        "\n",
        "- **LangChain Documentation**: https://python.langchain.com/\n",
        "- **LangChain Hub**: https://smith.langchain.com/hub\n",
        "- **LangSmith**: https://smith.langchain.com/\n",
        "- **GitHub**: https://github.com/langchain-ai/langchain\n",
        "\n",
        "---\n",
        "\n",
        "**Congratulations!** You've learned the fundamentals of the LangChain framework. Practice building your own applications and explore the advanced features!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cleanup: Remove temporary files\n",
        "import os\n",
        "if os.path.exists(\"sample_ml.txt\"):\n",
        "    os.remove(\"sample_ml.txt\")\n",
        "    print(\"✅ Cleaned up temporary files\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llm-learning",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
