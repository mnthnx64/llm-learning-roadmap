{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Retrieval Augmented Generation (RAG)\n",
        "\n",
        "This notebook covers Retrieval Augmented Generation (RAG) - a powerful technique for enhancing LLM responses with external knowledge:\n",
        "- **What is RAG?**: Combining retrieval of relevant information with LLM generation\n",
        "- **Why RAG?**: Overcome LLM limitations like knowledge cutoff, hallucinations, and lack of domain-specific knowledge\n",
        "- **RAG Components**: Document loading, chunking, embedding, vector storage, retrieval, and generation\n",
        "- **Building RAG Systems**: End-to-end implementation using LangChain and vector databases\n",
        "- **Advanced Techniques**: Query rewriting, reranking, and hybrid search\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "- Understand what RAG is and why it's important\n",
        "- Learn the components and architecture of RAG systems\n",
        "- Build a complete RAG pipeline from scratch\n",
        "- Implement document processing, chunking, and embedding\n",
        "- Use vector databases for efficient retrieval\n",
        "- Combine retrieval with LLM generation\n",
        "- Apply RAG to real-world use cases\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation\n",
        "\n",
        "Run this cell to install required packages (uncomment if needed):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install packages (uncomment if needed)\n",
        "# !pip install langchain langchain-openai langchain-community chromadb sentence-transformers pypdf python-dotenv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. What is RAG?\n",
        "\n",
        "**Retrieval Augmented Generation (RAG)** is a technique that enhances Large Language Models (LLMs) by:\n",
        "\n",
        "1. **Retrieving** relevant information from external knowledge sources (documents, databases, etc.)\n",
        "2. **Augmenting** the LLM's context with this retrieved information\n",
        "3. **Generating** responses based on both the LLM's training and the retrieved context\n",
        "\n",
        "### Why RAG?\n",
        "\n",
        "LLMs have several limitations:\n",
        "- **Knowledge Cutoff**: Training data has a cutoff date, missing recent information\n",
        "- **Hallucinations**: May generate plausible but incorrect information\n",
        "- **Domain-Specific Knowledge**: Limited knowledge in specialized domains\n",
        "- **Static Knowledge**: Cannot access real-time or private information\n",
        "\n",
        "### RAG Benefits:\n",
        "\n",
        "✅ **Up-to-date Information**: Access current information beyond training cutoff  \n",
        "✅ **Reduced Hallucinations**: Ground responses in retrieved documents  \n",
        "✅ **Domain Expertise**: Incorporate specialized knowledge bases  \n",
        "✅ **Transparency**: Can cite sources for generated answers  \n",
        "✅ **Cost-Effective**: No need to retrain models for new knowledge\n",
        "\n",
        "### RAG Architecture:\n",
        "\n",
        "```\n",
        "User Query\n",
        "    ↓\n",
        "Query Embedding\n",
        "    ↓\n",
        "Vector Search (Vector DB)\n",
        "    ↓\n",
        "Retrieve Relevant Documents\n",
        "    ↓\n",
        "Augment Prompt with Context\n",
        "    ↓\n",
        "LLM Generation\n",
        "    ↓\n",
        "Response\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain_community.document_loaders import TextLoader, PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. RAG Components\n",
        "\n",
        "A RAG system consists of several key components:\n",
        "\n",
        "### 2.1 Document Loading\n",
        "- Load documents from various sources (PDFs, text files, web pages, databases)\n",
        "- Extract text content from different formats\n",
        "\n",
        "### 2.2 Text Chunking\n",
        "- Split documents into smaller chunks\n",
        "- Balance chunk size: too small (loses context) vs too large (inefficient retrieval)\n",
        "- Common strategies: fixed-size, sentence-aware, semantic chunking\n",
        "\n",
        "### 2.3 Embedding Generation\n",
        "- Convert text chunks into vector embeddings\n",
        "- Use embedding models (e.g., OpenAI, Sentence Transformers)\n",
        "- Embeddings capture semantic meaning\n",
        "\n",
        "### 2.4 Vector Storage\n",
        "- Store embeddings in vector databases (ChromaDB, FAISS, Pinecone)\n",
        "- Enable fast similarity search\n",
        "\n",
        "### 2.5 Retrieval\n",
        "- Given a query, find most similar document chunks\n",
        "- Use similarity metrics (cosine similarity, dot product, etc.)\n",
        "- Can retrieve top-k most relevant chunks\n",
        "\n",
        "### 2.6 Generation\n",
        "- Augment LLM prompt with retrieved context\n",
        "- Generate response based on query + context\n",
        "- Optionally cite sources\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Building a Simple RAG System\n",
        "\n",
        "Let's build a complete RAG system step by step:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Step 1: Load Documents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 1 document(s)\n",
            "Document length: 1181 characters\n",
            "\n",
            "First 200 characters:\n",
            "\n",
            "Machine Learning is a subset of artificial intelligence that focuses on the development of algorithms \n",
            "and statistical models that enable computer systems to improve their performance on a specific t...\n"
          ]
        }
      ],
      "source": [
        "# Example: Create a sample document for demonstration\n",
        "sample_text = \"\"\"\n",
        "Machine Learning is a subset of artificial intelligence that focuses on the development of algorithms \n",
        "and statistical models that enable computer systems to improve their performance on a specific task \n",
        "through experience. Unlike traditional programming where explicit instructions are provided, machine \n",
        "learning systems learn patterns from data.\n",
        "\n",
        "Deep Learning is a specialized subset of machine learning that uses neural networks with multiple \n",
        "layers (hence \"deep\") to model and understand complex patterns in data. Deep learning has been \n",
        "particularly successful in areas like image recognition, natural language processing, and speech recognition.\n",
        "\n",
        "Natural Language Processing (NLP) is a branch of AI that helps computers understand, interpret, and \n",
        "manipulate human language. NLP combines computational linguistics with statistical, machine learning, \n",
        "and deep learning models to process human language in text or voice form.\n",
        "\n",
        "Vector databases are specialized databases designed to store and efficiently search high-dimensional \n",
        "vectors (embeddings). They enable similarity search, which is crucial for applications like RAG, \n",
        "semantic search, and recommendation systems.\n",
        "\"\"\"\n",
        "\n",
        "# Save to a temporary file for demonstration\n",
        "with open(\"sample_doc.txt\", \"w\") as f:\n",
        "    f.write(sample_text)\n",
        "\n",
        "# Load the document\n",
        "loader = TextLoader(\"sample_doc.txt\")\n",
        "documents = loader.load()\n",
        "\n",
        "print(f\"Loaded {len(documents)} document(s)\")\n",
        "print(f\"Document length: {len(documents[0].page_content)} characters\")\n",
        "print(f\"\\nFirst 200 characters:\\n{documents[0].page_content[:200]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Step 2: Chunk Documents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Split into 9 chunks\n",
            "\n",
            "Chunk sizes: [101, 99, 144, 193, 109, 99, 176, 100, 142]\n",
            "\n",
            "First chunk:\n",
            "Machine Learning is a subset of artificial intelligence that focuses on the development of algorithms\n"
          ]
        }
      ],
      "source": [
        "# Split documents into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=200,  # Characters per chunk\n",
        "    chunk_overlap=50,  # Overlap between chunks to preserve context\n",
        "    length_function=len,\n",
        ")\n",
        "\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f\"Split into {len(chunks)} chunks\")\n",
        "print(f\"\\nChunk sizes: {[len(chunk.page_content) for chunk in chunks]}\")\n",
        "print(f\"\\nFirst chunk:\\n{chunks[0].page_content}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Step 3: Create Embeddings and Vector Store\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⚠️  OPENAI_API_KEY not found in environment variables.\n",
            "Please set your OpenAI API key to use embeddings.\n",
            "You can use: export OPENAI_API_KEY='your-key-here'\n",
            "\n",
            "For now, we'll continue with a placeholder structure.\n"
          ]
        }
      ],
      "source": [
        "# Initialize embeddings (using OpenAI - requires API key)\n",
        "# For demonstration, we'll check if API key is available\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "if api_key:\n",
        "    embeddings = OpenAIEmbeddings(openai_api_key=api_key)\n",
        "    \n",
        "    # Create vector store from chunks\n",
        "    vectorstore = Chroma.from_documents(\n",
        "        documents=chunks,\n",
        "        embedding=embeddings,\n",
        "        persist_directory=\"./rag_chroma_db\"  # Persist to disk\n",
        "    )\n",
        "    \n",
        "    print(\"Vector store created successfully!\")\n",
        "    print(f\"Number of vectors stored: {len(chunks)}\")\n",
        "else:\n",
        "    print(\"⚠️  OPENAI_API_KEY not found in environment variables.\")\n",
        "    print(\"Please set your OpenAI API key to use embeddings.\")\n",
        "    print(\"You can use: export OPENAI_API_KEY='your-key-here'\")\n",
        "    print(\"\\nFor now, we'll continue with a placeholder structure.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.4 Step 4: Create Retrieval Chain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⚠️  Skipping chain creation - API key required\n"
          ]
        }
      ],
      "source": [
        "# Create a retrieval chain that combines retrieval and generation\n",
        "if api_key:\n",
        "    # Initialize LLM\n",
        "    llm = ChatOpenAI(\n",
        "        model_name=\"gpt-3.5-turbo\",\n",
        "        temperature=0,\n",
        "        openai_api_key=api_key\n",
        "    )\n",
        "    \n",
        "    # Create a custom prompt template\n",
        "    prompt_template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
        "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "    \n",
        "    Context: {context}\n",
        "    \n",
        "    Question: {question}\n",
        "    \n",
        "    Answer:\"\"\"\n",
        "    \n",
        "    PROMPT = PromptTemplate(\n",
        "        template=prompt_template,\n",
        "        input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "    \n",
        "    # Create retrieval QA chain\n",
        "    qa_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",  # \"stuff\", \"map_reduce\", \"refine\", \"map_rerank\"\n",
        "        retriever=vectorstore.as_retriever(search_kwargs={\"k\": 2}),  # Retrieve top 2 chunks\n",
        "        chain_type_kwargs={\"prompt\": PROMPT},\n",
        "        return_source_documents=True\n",
        "    )\n",
        "    \n",
        "    print(\"RAG chain created successfully!\")\n",
        "else:\n",
        "    print(\"⚠️  Skipping chain creation - API key required\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.5 Step 5: Query the RAG System\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⚠️  API key required to run queries\n",
            "\n",
            "Example of what the RAG system would do:\n",
            "1. Convert query to embedding\n",
            "2. Search vector store for similar chunks\n",
            "3. Retrieve top-k relevant chunks\n",
            "4. Augment LLM prompt with retrieved context\n",
            "5. Generate answer based on query + context\n"
          ]
        }
      ],
      "source": [
        "# Example queries\n",
        "if api_key:\n",
        "    queries = [\n",
        "        \"What is machine learning?\",\n",
        "        \"How does deep learning differ from machine learning?\",\n",
        "        \"What are vector databases used for?\"\n",
        "    ]\n",
        "    \n",
        "    for query in queries:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Query: {query}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        result = qa_chain.invoke({\"query\": query})\n",
        "        \n",
        "        print(f\"\\nAnswer: {result['result']}\")\n",
        "        print(f\"\\nSource documents retrieved: {len(result['source_documents'])}\")\n",
        "        if result['source_documents']:\n",
        "            print(f\"\\nFirst source chunk preview:\")\n",
        "            print(result['source_documents'][0].page_content[:200] + \"...\")\n",
        "else:\n",
        "    print(\"⚠️  API key required to run queries\")\n",
        "    print(\"\\nExample of what the RAG system would do:\")\n",
        "    print(\"1. Convert query to embedding\")\n",
        "    print(\"2. Search vector store for similar chunks\")\n",
        "    print(\"3. Retrieve top-k relevant chunks\")\n",
        "    print(\"4. Augment LLM prompt with retrieved context\")\n",
        "    print(\"5. Generate answer based on query + context\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Advanced RAG Techniques\n",
        "\n",
        "### 4.1 Query Rewriting\n",
        "- Rephrase queries to improve retrieval\n",
        "- Generate multiple query variations\n",
        "- Use query expansion techniques\n",
        "\n",
        "### 4.2 Reranking\n",
        "- Initial retrieval gets many candidates\n",
        "- Rerank using more sophisticated models (cross-encoders)\n",
        "- Improve precision of retrieved documents\n",
        "\n",
        "### 4.3 Hybrid Search\n",
        "- Combine semantic search (embeddings) with keyword search (BM25)\n",
        "- Get benefits of both approaches\n",
        "- Weighted combination of results\n",
        "\n",
        "### 4.4 Chunking Strategies\n",
        "- **Fixed-size**: Simple but may break context\n",
        "- **Sentence-aware**: Split at sentence boundaries\n",
        "- **Semantic chunking**: Group semantically related content\n",
        "- **Sliding window**: Overlap chunks to preserve context\n",
        "\n",
        "### 4.5 Metadata Filtering\n",
        "- Filter documents by metadata (date, source, category)\n",
        "- Improve retrieval relevance\n",
        "- Support multi-tenant scenarios\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. RAG Best Practices\n",
        "\n",
        "### Document Processing\n",
        "- ✅ Clean and normalize text\n",
        "- ✅ Remove irrelevant content\n",
        "- ✅ Preserve important formatting\n",
        "- ✅ Handle multiple languages\n",
        "\n",
        "### Chunking\n",
        "- ✅ Choose appropriate chunk size (typically 200-1000 tokens)\n",
        "- ✅ Use overlap to preserve context\n",
        "- ✅ Consider document structure (paragraphs, sections)\n",
        "- ✅ Test different chunking strategies\n",
        "\n",
        "### Retrieval\n",
        "- ✅ Tune number of retrieved chunks (k)\n",
        "- ✅ Use appropriate similarity metric\n",
        "- ✅ Consider reranking for better precision\n",
        "- ✅ Implement metadata filtering when needed\n",
        "\n",
        "### Generation\n",
        "- ✅ Design clear prompt templates\n",
        "- ✅ Include instructions for using context\n",
        "- ✅ Handle cases where no relevant context is found\n",
        "- ✅ Enable source citation\n",
        "\n",
        "### Evaluation\n",
        "- ✅ Measure retrieval quality (precision, recall)\n",
        "- ✅ Evaluate answer quality (accuracy, relevance)\n",
        "- ✅ Test with diverse queries\n",
        "- ✅ Monitor for hallucinations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Common RAG Challenges and Solutions\n",
        "\n",
        "### Challenge 1: Irrelevant Retrieval\n",
        "**Problem**: Retrieved chunks don't match the query  \n",
        "**Solutions**: \n",
        "- Improve query understanding (query rewriting)\n",
        "- Use better embedding models\n",
        "- Implement reranking\n",
        "- Tune retrieval parameters\n",
        "\n",
        "### Challenge 2: Context Window Limits\n",
        "**Problem**: Too many retrieved chunks exceed LLM context window  \n",
        "**Solutions**:\n",
        "- Limit number of retrieved chunks\n",
        "- Use compression techniques\n",
        "- Implement hierarchical retrieval\n",
        "- Use chain types like \"map_reduce\"\n",
        "\n",
        "### Challenge 3: Outdated Information\n",
        "**Problem**: Vector store contains outdated information  \n",
        "**Solutions**:\n",
        "- Implement incremental updates\n",
        "- Use versioning for documents\n",
        "- Add timestamp metadata\n",
        "- Periodic re-indexing\n",
        "\n",
        "### Challenge 4: Hallucinations\n",
        "**Problem**: LLM generates information not in retrieved context  \n",
        "**Solutions**:\n",
        "- Improve prompt instructions\n",
        "- Use temperature=0 for more deterministic outputs\n",
        "- Implement answer validation\n",
        "- Add source citation requirements\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Next Steps\n",
        "\n",
        "Now that you understand RAG fundamentals:\n",
        "\n",
        "1. **Experiment with different chunking strategies** - Try various chunk sizes and overlap\n",
        "2. **Test different embedding models** - Compare OpenAI, Sentence Transformers, etc.\n",
        "3. **Explore advanced techniques** - Implement reranking, query rewriting, hybrid search\n",
        "4. **Build domain-specific RAG** - Apply to your own documents and use cases\n",
        "5. **Integrate with LangChain** - Explore LangChain's RAG capabilities and tools\n",
        "6. **Evaluate your system** - Measure retrieval and generation quality\n",
        "\n",
        "### Resources\n",
        "- [LangChain RAG Documentation](https://python.langchain.com/docs/use_cases/question_answering/)\n",
        "- [ChromaDB Documentation](https://docs.trychroma.com/)\n",
        "- [FAISS Documentation](https://github.com/facebookresearch/faiss)\n",
        "- Research Papers: \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\" (Lewis et al., 2020)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Temporary file cleaned up!\n"
          ]
        }
      ],
      "source": [
        "# Cleanup: Remove temporary file\n",
        "import os\n",
        "if os.path.exists(\"sample_doc.txt\"):\n",
        "    os.remove(\"sample_doc.txt\")\n",
        "    print(\"Temporary file cleaned up!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llm-learning",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
