{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Vector Databases: FAISS and ChromaDB\n",
        "\n",
        "This notebook covers Vector Databases for efficient similarity search:\n",
        "- **What are Vector DBs?**: Specialized databases for storing and searching high-dimensional vectors\n",
        "- **FAISS**: Facebook AI Similarity Search - fast similarity search library\n",
        "- **ChromaDB**: Open-source embedding database with built-in features\n",
        "- **Use Cases**: RAG, semantic search, recommendation systems, and more\n",
        "- **Practical Examples**: Building and querying vector databases\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "- Understand what vector databases are and why they're needed\n",
        "- Learn to use FAISS for efficient similarity search\n",
        "- Learn to use ChromaDB for embedding storage and retrieval\n",
        "- Compare different vector database approaches\n",
        "- Apply vector DBs for real-world scenarios\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation\n",
        "\n",
        "Run this cell to install required packages (uncomment if needed):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install packages (uncomment if needed)\n",
        "# !pip install faiss-cpu chromadb sentence-transformers numpy pandas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. What are Vector Databases?\n",
        "\n",
        "**Vector Databases** are specialized databases designed to store and efficiently search high-dimensional vectors (embeddings). Unlike traditional databases that search by exact matches, vector DBs enable:\n",
        "\n",
        "- **Similarity Search**: Find vectors similar to a query vector\n",
        "- **Scalability**: Handle millions or billions of vectors efficiently\n",
        "- **Speed**: Optimized algorithms for fast nearest neighbor search\n",
        "- **Metadata Storage**: Store additional information alongside vectors\n",
        "\n",
        "### Why Vector DBs?\n",
        "\n",
        "When you have thousands or millions of embeddings, calculating similarity with all vectors becomes slow. Vector DBs use:\n",
        "- **Indexing**: Special data structures (IVF, HNSW, etc.) for fast search\n",
        "- **Approximate Search**: Trade some accuracy for significant speed gains\n",
        "- **GPU Support**: Leverage GPU acceleration for faster searches\n",
        "\n",
        "### Common Use Cases:\n",
        "- **RAG (Retrieval Augmented Generation)**: Find relevant documents for LLM context\n",
        "- **Semantic Search**: Search by meaning, not keywords\n",
        "- **Recommendation Systems**: Find similar items/users\n",
        "- **Deduplication**: Find duplicate or similar content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import os\n",
        "import time\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. FAISS (Facebook AI Similarity Search)\n",
        "\n",
        "**FAISS** is a library developed by Facebook AI Research for efficient similarity search and clustering of dense vectors. It's:\n",
        "\n",
        "- **Fast**: Optimized C++ implementation with Python bindings\n",
        "- **Scalable**: Handles billions of vectors\n",
        "- **Flexible**: Multiple indexing methods (exact, approximate)\n",
        "- **Lightweight**: No external dependencies for basic usage\n",
        "\n",
        "### Key Features:\n",
        "- Multiple index types (Flat, IVF, HNSW, etc.)\n",
        "- GPU support\n",
        "- Batch operations\n",
        "- Memory-efficient\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded: 384 dimensions\n"
          ]
        }
      ],
      "source": [
        "# Load embedding model\n",
        "try:\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    print(f\"Model loaded: {model.get_sentence_embedding_dimension()} dimensions\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    model = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating embeddings...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3fae5ec8d860404c9d6ed50af9ac66ea",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated 10 embeddings of dimension 384\n",
            "\n",
            "FAISS index created with 10 vectors\n",
            "Index type: IndexFlatL2\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import faiss\n",
        "    \n",
        "    # Sample documents\n",
        "    documents = [\n",
        "        \"Machine learning is a subset of artificial intelligence\",\n",
        "        \"Deep learning uses neural networks with multiple layers\",\n",
        "        \"Natural language processing helps computers understand human language\",\n",
        "        \"Computer vision enables machines to interpret visual information\",\n",
        "        \"Reinforcement learning trains agents through rewards and penalties\",\n",
        "        \"Supervised learning uses labeled data to train models\",\n",
        "        \"Unsupervised learning finds patterns in unlabeled data\",\n",
        "        \"I love eating pizza on weekends\",\n",
        "        \"The weather is beautiful today\",\n",
        "        \"Python is a popular programming language for data science\"\n",
        "    ]\n",
        "    \n",
        "    if model:\n",
        "        # Generate embeddings\n",
        "        print(\"Generating embeddings...\")\n",
        "        embeddings = model.encode(documents, show_progress_bar=True)\n",
        "        dimension = embeddings.shape[1]\n",
        "        print(f\"Generated {len(documents)} embeddings of dimension {dimension}\")\n",
        "        \n",
        "        # Normalize embeddings for cosine similarity (FAISS uses L2 distance)\n",
        "        # For cosine similarity, we normalize vectors\n",
        "        faiss.normalize_L2(embeddings)\n",
        "        \n",
        "        # Create FAISS index\n",
        "        # IndexFlatL2: Exact search using L2 (Euclidean) distance\n",
        "        # For normalized vectors, L2 distance ≈ cosine distance\n",
        "        index = faiss.IndexFlatL2(dimension)\n",
        "        \n",
        "        # Add vectors to index\n",
        "        index.add(embeddings.astype('float32'))\n",
        "        \n",
        "        print(f\"\\nFAISS index created with {index.ntotal} vectors\")\n",
        "        print(f\"Index type: {type(index).__name__}\")\n",
        "        \n",
        "    else:\n",
        "        print(\"Model not available\")\n",
        "        index = None\n",
        "        embeddings = None\n",
        "        \n",
        "except ImportError:\n",
        "    print(\"FAISS not installed. Install with: pip install faiss-cpu\")\n",
        "    index = None\n",
        "    embeddings = None\n",
        "    documents = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 FAISS - Basic Example with Flat Index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: 'How do neural networks learn?'\n",
            "\n",
            "Top 3 Most Similar Documents:\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Rank 1 (Distance: 0.9683):\n",
            "  Deep learning uses neural networks with multiple layers\n",
            "\n",
            "Rank 2 (Distance: 1.1865):\n",
            "  Machine learning is a subset of artificial intelligence\n",
            "\n",
            "Rank 3 (Distance: 1.2397):\n",
            "  Supervised learning uses labeled data to train models\n"
          ]
        }
      ],
      "source": [
        "# Search in FAISS index\n",
        "if index is not None and model is not None:\n",
        "    # Query\n",
        "    query = \"How do neural networks learn?\"\n",
        "    \n",
        "    # Generate query embedding\n",
        "    query_embedding = model.encode([query])\n",
        "    faiss.normalize_L2(query_embedding)\n",
        "    query_embedding = query_embedding.astype('float32')\n",
        "    \n",
        "    # Search for top 3 similar vectors\n",
        "    k = 3\n",
        "    distances, indices = index.search(query_embedding, k)\n",
        "    \n",
        "    print(f\"Query: '{query}'\\n\")\n",
        "    print(\"Top 3 Most Similar Documents:\")\n",
        "    print(\"-\" * 70)\n",
        "    \n",
        "    for i, (idx, dist) in enumerate(zip(indices[0], distances[0]), 1):\n",
        "        print(f\"\\nRank {i} (Distance: {dist:.4f}):\")\n",
        "        print(f\"  {documents[idx]}\")\n",
        "        \n",
        "else:\n",
        "    print(\"Index not available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 FAISS - Approximate Search with IVF Index\n",
        "\n",
        "For larger datasets, exact search becomes slow. FAISS provides approximate search methods that trade some accuracy for significant speed improvements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training IVF index...\n",
            "IVF Index created with 10 vectors\n",
            "Number of clusters (nlist): 4\n",
            "Number of probes: 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING clustering 10 points to 4 centroids: please provide at least 156 training points\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Search Performance (for 10 vectors):\n",
            "Exact search (Flat): 0.538 ms\n",
            "Approximate search (IVF): 0.481 ms\n"
          ]
        }
      ],
      "source": [
        "if embeddings is not None:\n",
        "    try:\n",
        "        dimension = embeddings.shape[1]\n",
        "        n_vectors = len(embeddings)\n",
        "        \n",
        "        # Create IVF (Inverted File Index) for approximate search\n",
        "        # nlist: number of clusters (centroids)\n",
        "        nlist = min(4, n_vectors // 2)  # Use 4 clusters for small dataset\n",
        "        \n",
        "        # Quantizer for clustering\n",
        "        quantizer = faiss.IndexFlatL2(dimension)\n",
        "        \n",
        "        # IVF index\n",
        "        index_ivf = faiss.IndexIVFFlat(quantizer, dimension, nlist)\n",
        "        \n",
        "        # Train the index (required for IVF)\n",
        "        print(\"Training IVF index...\")\n",
        "        index_ivf.train(embeddings.astype('float32'))\n",
        "        \n",
        "        # Add vectors\n",
        "        index_ivf.add(embeddings.astype('float32'))\n",
        "        \n",
        "        # Set number of probes (how many clusters to search)\n",
        "        index_ivf.nprobe = 2\n",
        "        \n",
        "        print(f\"IVF Index created with {index_ivf.ntotal} vectors\")\n",
        "        print(f\"Number of clusters (nlist): {nlist}\")\n",
        "        print(f\"Number of probes: {index_ivf.nprobe}\")\n",
        "        \n",
        "        # Compare search speed\n",
        "        if model:\n",
        "            query_embedding = model.encode([\"neural networks and deep learning\"])\n",
        "            faiss.normalize_L2(query_embedding)\n",
        "            query_embedding = query_embedding.astype('float32')\n",
        "            \n",
        "            # Time exact search\n",
        "            start = time.time()\n",
        "            distances_exact, indices_exact = index.search(query_embedding, 3)\n",
        "            time_exact = time.time() - start\n",
        "            \n",
        "            # Time approximate search\n",
        "            start = time.time()\n",
        "            distances_approx, indices_approx = index_ivf.search(query_embedding, 3)\n",
        "            time_approx = time.time() - start\n",
        "            \n",
        "            print(f\"\\nSearch Performance (for {n_vectors} vectors):\")\n",
        "            print(f\"Exact search (Flat): {time_exact*1000:.3f} ms\")\n",
        "            print(f\"Approximate search (IVF): {time_approx*1000:.3f} ms\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"Error creating IVF index: {e}\")\n",
        "        print(\"Note: IVF is more useful for larger datasets (thousands+ vectors)\")\n",
        "else:\n",
        "    print(\"Embeddings not available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 FAISS - Saving and Loading Index\n",
        "\n",
        "FAISS allows you to save indexes to disk for persistence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index saved to faiss_index.bin\n",
            "Index loaded: 10 vectors\n",
            "\n",
            "Search test with loaded index:\n",
            "  1. Machine learning is a subset of artificial intelligence\n",
            "  2. Natural language processing helps computers understand human language\n",
            "\n",
            "Cleaned up faiss_index.bin\n"
          ]
        }
      ],
      "source": [
        "if index is not None:\n",
        "    try:\n",
        "        # Save index to disk\n",
        "        index_path = \"faiss_index.bin\"\n",
        "        faiss.write_index(index, index_path)\n",
        "        print(f\"Index saved to {index_path}\")\n",
        "        \n",
        "        # Load index from disk\n",
        "        loaded_index = faiss.read_index(index_path)\n",
        "        print(f\"Index loaded: {loaded_index.ntotal} vectors\")\n",
        "        \n",
        "        # Verify it works\n",
        "        if model:\n",
        "            query_embedding = model.encode([\"artificial intelligence\"])\n",
        "            faiss.normalize_L2(query_embedding)\n",
        "            query_embedding = query_embedding.astype('float32')\n",
        "            \n",
        "            distances, indices = loaded_index.search(query_embedding, 2)\n",
        "            print(f\"\\nSearch test with loaded index:\")\n",
        "            for i, idx in enumerate(indices[0], 1):\n",
        "                print(f\"  {i}. {documents[idx]}\")\n",
        "        \n",
        "        # Clean up\n",
        "        if os.path.exists(index_path):\n",
        "            os.remove(index_path)\n",
        "            print(f\"\\nCleaned up {index_path}\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"Error saving/loading index: {e}\")\n",
        "else:\n",
        "    print(\"Index not available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. ChromaDB\n",
        "\n",
        "**ChromaDB** is an open-source embedding database that provides:\n",
        "- **Easy to use**: Simple Python API\n",
        "- **Built-in features**: Automatic embedding generation, metadata filtering\n",
        "- **Persistence**: Save collections to disk\n",
        "- **Production-ready**: Designed for real applications\n",
        "\n",
        "### Key Features:\n",
        "- Automatic embedding generation (or use your own)\n",
        "- Metadata filtering and querying\n",
        "- Collection management\n",
        "- Persistent storage\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 ChromaDB - Basic Example\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChromaDB collection created successfully!\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import chromadb\n",
        "    from chromadb.config import Settings\n",
        "    \n",
        "    # Initialize ChromaDB client\n",
        "    # Using in-memory mode for this example\n",
        "    # For persistence: client = chromadb.Client()\n",
        "    client = chromadb.Client(Settings(anonymized_telemetry=False))\n",
        "    \n",
        "    # Create or get a collection\n",
        "    # ChromaDB can automatically generate embeddings, but we'll use our own\n",
        "    collection = client.create_collection(\n",
        "        name=\"ai_documents\",\n",
        "        metadata={\"description\": \"AI and ML related documents\"}\n",
        "    )\n",
        "    \n",
        "    print(\"ChromaDB collection created successfully!\")\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"ChromaDB not installed. Install with: pip install chromadb\")\n",
        "    client = None\n",
        "    collection = None\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing ChromaDB: {e}\")\n",
        "    client = None\n",
        "    collection = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added 10 documents to ChromaDB collection\n",
            "Collection count: 10\n"
          ]
        }
      ],
      "source": [
        "# Add documents to ChromaDB\n",
        "if collection is not None and model is not None:\n",
        "    # Generate embeddings\n",
        "    embeddings_list = model.encode(documents).tolist()\n",
        "    \n",
        "    # Add documents with embeddings and metadata\n",
        "    collection.add(\n",
        "        embeddings=embeddings_list,\n",
        "        documents=documents,\n",
        "        ids=[f\"doc_{i}\" for i in range(len(documents))],\n",
        "        metadatas=[\n",
        "            {\"category\": \"AI\" if i < 7 else \"other\", \"index\": i}\n",
        "            for i in range(len(documents))\n",
        "        ]\n",
        "    )\n",
        "    \n",
        "    print(f\"Added {len(documents)} documents to ChromaDB collection\")\n",
        "    print(f\"Collection count: {collection.count()}\")\n",
        "else:\n",
        "    print(\"Collection or model not available\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: 'How do neural networks learn?'\n",
            "\n",
            "Top 3 Most Similar Documents:\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Rank 1 (ID: doc_1):\n",
            "  Deep learning uses neural networks with multiple layers\n",
            "  Metadata: {'index': 1, 'category': 'AI'}\n",
            "\n",
            "Rank 2 (ID: doc_0):\n",
            "  Machine learning is a subset of artificial intelligence\n",
            "  Metadata: {'category': 'AI', 'index': 0}\n",
            "\n",
            "Rank 3 (ID: doc_5):\n",
            "  Supervised learning uses labeled data to train models\n",
            "  Metadata: {'category': 'AI', 'index': 5}\n"
          ]
        }
      ],
      "source": [
        "# Query ChromaDB\n",
        "if collection is not None and model is not None:\n",
        "    query = \"How do neural networks learn?\"\n",
        "    \n",
        "    # Generate query embedding\n",
        "    query_embedding = model.encode([query]).tolist()\n",
        "    \n",
        "    # Search\n",
        "    results = collection.query(\n",
        "        query_embeddings=query_embedding,\n",
        "        n_results=3\n",
        "    )\n",
        "    \n",
        "    print(f\"Query: '{query}'\\n\")\n",
        "    print(\"Top 3 Most Similar Documents:\")\n",
        "    print(\"-\" * 70)\n",
        "    \n",
        "    for i, (doc, metadata, doc_id) in enumerate(\n",
        "        zip(results['documents'][0], results['metadatas'][0], results['ids'][0]), \n",
        "        1\n",
        "    ):\n",
        "        print(f\"\\nRank {i} (ID: {doc_id}):\")\n",
        "        print(f\"  {doc}\")\n",
        "        print(f\"  Metadata: {metadata}\")\n",
        "        \n",
        "else:\n",
        "    print(\"Collection or model not available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 ChromaDB - Using Built-in Embeddings\n",
        "\n",
        "ChromaDB can automatically generate embeddings using default models, making it even easier to use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/mx98/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz: 100%|██████████| 79.3M/79.3M [00:02<00:00, 33.5MiB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added 5 documents with auto-generated embeddings\n",
            "\n",
            "Query Results (using text query):\n",
            "----------------------------------------------------------------------\n",
            "1. Machine learning is a subset of artificial intelligence\n",
            "2. Deep learning uses neural networks with multiple layers\n"
          ]
        }
      ],
      "source": [
        "if client is not None:\n",
        "    try:\n",
        "        # Create collection with default embedding function\n",
        "        # ChromaDB uses sentence-transformers by default\n",
        "        collection_auto = client.create_collection(\n",
        "            name=\"auto_embeddings\",\n",
        "            metadata={\"description\": \"Collection with automatic embeddings\"}\n",
        "        )\n",
        "        \n",
        "        # Add documents without providing embeddings\n",
        "        # ChromaDB will generate them automatically\n",
        "        collection_auto.add(\n",
        "            documents=documents[:5],  # Use first 5 documents\n",
        "            ids=[f\"auto_doc_{i}\" for i in range(5)],\n",
        "            metadatas=[\n",
        "                {\"category\": \"AI\", \"index\": i}\n",
        "                for i in range(5)\n",
        "            ]\n",
        "        )\n",
        "        \n",
        "        print(f\"Added {collection_auto.count()} documents with auto-generated embeddings\")\n",
        "        \n",
        "        # Query using text directly (no need to generate embeddings)\n",
        "        results = collection_auto.query(\n",
        "            query_texts=[\"neural networks and machine learning\"],\n",
        "            n_results=2\n",
        "        )\n",
        "        \n",
        "        print(\"\\nQuery Results (using text query):\")\n",
        "        print(\"-\" * 70)\n",
        "        for i, doc in enumerate(results['documents'][0], 1):\n",
        "            print(f\"{i}. {doc}\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "else:\n",
        "    print(\"ChromaDB client not available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 ChromaDB - Metadata Filtering\n",
        "\n",
        "One of ChromaDB's powerful features is filtering by metadata.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query Results (Filtered: category='AI'):\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "1. Machine learning is a subset of artificial intelligence\n",
            "   Category: AI\n",
            "\n",
            "2. Supervised learning uses labeled data to train models\n",
            "   Category: AI\n",
            "\n",
            "3. Computer vision enables machines to interpret visual information\n",
            "   Category: AI\n",
            "\n",
            "4. Deep learning uses neural networks with multiple layers\n",
            "   Category: AI\n",
            "\n",
            "5. Unsupervised learning finds patterns in unlabeled data\n",
            "   Category: AI\n",
            "\n",
            "\n",
            "Query Results (No Filter):\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "1. Machine learning is a subset of artificial intelligence\n",
            "   Category: AI\n",
            "\n",
            "2. Supervised learning uses labeled data to train models\n",
            "   Category: AI\n",
            "\n",
            "3. Computer vision enables machines to interpret visual information\n",
            "   Category: AI\n",
            "\n",
            "4. Deep learning uses neural networks with multiple layers\n",
            "   Category: AI\n",
            "\n",
            "5. Unsupervised learning finds patterns in unlabeled data\n",
            "   Category: AI\n"
          ]
        }
      ],
      "source": [
        "if collection is not None and model is not None:\n",
        "    query_embedding = model.encode([\"machine learning techniques\"]).tolist()\n",
        "    \n",
        "    # Query with metadata filter - only AI category documents\n",
        "    results_filtered = collection.query(\n",
        "        query_embeddings=query_embedding,\n",
        "        n_results=5,\n",
        "        where={\"category\": \"AI\"}  # Filter by metadata\n",
        "    )\n",
        "    \n",
        "    print(\"Query Results (Filtered: category='AI'):\")\n",
        "    print(\"-\" * 70)\n",
        "    for i, (doc, metadata) in enumerate(\n",
        "        zip(results_filtered['documents'][0], results_filtered['metadatas'][0]), \n",
        "        1\n",
        "    ):\n",
        "        print(f\"\\n{i}. {doc}\")\n",
        "        print(f\"   Category: {metadata['category']}\")\n",
        "    \n",
        "    # Compare with unfiltered results\n",
        "    results_unfiltered = collection.query(\n",
        "        query_embeddings=query_embedding,\n",
        "        n_results=5\n",
        "    )\n",
        "    \n",
        "    print(\"\\n\\nQuery Results (No Filter):\")\n",
        "    print(\"-\" * 70)\n",
        "    for i, (doc, metadata) in enumerate(\n",
        "        zip(results_unfiltered['documents'][0], results_unfiltered['metadatas'][0]), \n",
        "        1\n",
        "    ):\n",
        "        print(f\"\\n{i}. {doc}\")\n",
        "        print(f\"   Category: {metadata['category']}\")\n",
        "        \n",
        "else:\n",
        "    print(\"Collection or model not available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.4 ChromaDB - Persistent Storage\n",
        "\n",
        "ChromaDB can save collections to disk for persistence across sessions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added 5 documents to persistent collection\n",
            "\n",
            "Query Results from Persistent Collection:\n",
            "1. Machine learning is a subset of artificial intelligence\n",
            "2. Natural language processing helps computers understand human language\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    # Create persistent client\n",
        "    persistent_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "    \n",
        "    # Create or get collection\n",
        "    persistent_collection = persistent_client.get_or_create_collection(\n",
        "        name=\"persistent_docs\"\n",
        "    )\n",
        "    \n",
        "    if persistent_collection.count() == 0:\n",
        "        # Add documents if collection is empty\n",
        "        if model:\n",
        "            embeddings_list = model.encode(documents[:5]).tolist()\n",
        "            persistent_collection.add(\n",
        "                embeddings=embeddings_list,\n",
        "                documents=documents[:5],\n",
        "                ids=[f\"persist_{i}\" for i in range(5)]\n",
        "            )\n",
        "            print(f\"Added {persistent_collection.count()} documents to persistent collection\")\n",
        "        else:\n",
        "            print(\"Model not available\")\n",
        "    else:\n",
        "        print(f\"Loaded existing collection with {persistent_collection.count()} documents\")\n",
        "    \n",
        "    # Query the persistent collection\n",
        "    if model:\n",
        "        query_embedding = model.encode([\"artificial intelligence\"]).tolist()\n",
        "        results = persistent_collection.query(\n",
        "            query_embeddings=query_embedding,\n",
        "            n_results=2\n",
        "        )\n",
        "        \n",
        "        print(\"\\nQuery Results from Persistent Collection:\")\n",
        "        for i, doc in enumerate(results['documents'][0], 1):\n",
        "            print(f\"{i}. {doc}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error with persistent storage: {e}\")\n",
        "    print(\"Note: Persistent storage saves data to './chroma_db' directory\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Comparison: FAISS vs ChromaDB\n",
        "\n",
        "### FAISS\n",
        "**Pros:**\n",
        "- Extremely fast and optimized\n",
        "- Handles billions of vectors\n",
        "- Multiple indexing algorithms\n",
        "- GPU support\n",
        "- Lightweight (just the search library)\n",
        "\n",
        "**Cons:**\n",
        "- Lower-level API (more code needed)\n",
        "- No built-in metadata filtering\n",
        "- No automatic embedding generation\n",
        "- Manual persistence management\n",
        "\n",
        "**Best for:** Large-scale similarity search, research, when you need maximum performance\n",
        "\n",
        "### ChromaDB\n",
        "**Pros:**\n",
        "- Easy-to-use high-level API\n",
        "- Built-in embedding generation\n",
        "- Metadata filtering and querying\n",
        "- Automatic persistence\n",
        "- Production-ready features\n",
        "\n",
        "**Cons:**\n",
        "- Less control over indexing algorithms\n",
        "- May be slower for very large datasets\n",
        "- More dependencies\n",
        "\n",
        "**Best for:** Production applications, RAG systems, when you need metadata filtering\n",
        "\n",
        "### When to Use Which?\n",
        "\n",
        "- **FAISS**: When you need maximum performance, have millions+ vectors, or want fine-grained control\n",
        "- **ChromaDB**: When you need metadata filtering, want easier setup, or building production RAG systems\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Practical Example: Building a Simple RAG System\n",
        "\n",
        "Let's build a simple RAG (Retrieval Augmented Generation) system using ChromaDB to demonstrate a real-world use case.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Knowledge base created with 8 documents\n",
            "\n",
            "User Query: 'How does deep learning work?'\n",
            "\n",
            "Retrieved Context (for LLM):\n",
            "======================================================================\n",
            "\n",
            "1. Deep learning uses neural networks with multiple hidden layers to learn complex patterns.\n",
            "\n",
            "2. Computer vision allows machines to interpret and understand visual information from images.\n",
            "\n",
            "3. Machine learning algorithms learn patterns from data without explicit programming.\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Context for LLM (would be sent to GPT/Claude/etc.):\n",
            "======================================================================\n",
            "\n",
            "Context:\n",
            "Deep learning uses neural networks with multiple hidden layers to learn complex patterns.\n",
            "\n",
            "Computer vision allows machines to interpret and understand visual information from images.\n",
            "\n",
            "Machine learning algorithms learn patterns from data without explicit programming.\n",
            "\n",
            "User Question: How does deep learning work?\n"
          ]
        }
      ],
      "source": [
        "# Simple RAG System Example\n",
        "if client is not None and model is not None:\n",
        "    # Create a knowledge base\n",
        "    knowledge_base = [\n",
        "        \"Machine learning algorithms learn patterns from data without explicit programming.\",\n",
        "        \"Deep learning uses neural networks with multiple hidden layers to learn complex patterns.\",\n",
        "        \"Natural language processing enables computers to understand and generate human language.\",\n",
        "        \"Computer vision allows machines to interpret and understand visual information from images.\",\n",
        "        \"Reinforcement learning trains agents to make decisions through trial and error with rewards.\",\n",
        "        \"Supervised learning requires labeled training data to learn input-output mappings.\",\n",
        "        \"Unsupervised learning discovers hidden patterns in data without labels.\",\n",
        "        \"Transfer learning allows models trained on one task to be adapted for related tasks.\"\n",
        "    ]\n",
        "    \n",
        "    # Create RAG collection\n",
        "    rag_collection = client.create_collection(name=\"rag_knowledge_base\")\n",
        "    \n",
        "    # Add knowledge base documents\n",
        "    rag_collection.add(\n",
        "        documents=knowledge_base,\n",
        "        ids=[f\"kb_{i}\" for i in range(len(knowledge_base))],\n",
        "        metadatas=[{\"source\": \"knowledge_base\", \"topic\": \"AI/ML\"} for _ in knowledge_base]\n",
        "    )\n",
        "    \n",
        "    print(f\"Knowledge base created with {rag_collection.count()} documents\")\n",
        "    \n",
        "    # Simulate a user query\n",
        "    user_query = \"How does deep learning work?\"\n",
        "    \n",
        "    # Retrieve relevant context\n",
        "    results = rag_collection.query(\n",
        "        query_texts=[user_query],\n",
        "        n_results=3\n",
        "    )\n",
        "    \n",
        "    print(f\"\\nUser Query: '{user_query}'\")\n",
        "    print(\"\\nRetrieved Context (for LLM):\")\n",
        "    print(\"=\" * 70)\n",
        "    for i, doc in enumerate(results['documents'][0], 1):\n",
        "        print(f\"\\n{i}. {doc}\")\n",
        "    \n",
        "    # In a real RAG system, you would:\n",
        "    # 1. Retrieve relevant documents (done above)\n",
        "    # 2. Combine them into context\n",
        "    # 3. Send context + query to LLM\n",
        "    # 4. Return LLM response\n",
        "    \n",
        "    context = \"\\n\\n\".join(results['documents'][0])\n",
        "    print(\"\\n\\n\" + \"=\" * 70)\n",
        "    print(\"Context for LLM (would be sent to GPT/Claude/etc.):\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"\\nContext:\\n{context}\\n\\nUser Question: {user_query}\")\n",
        "    \n",
        "else:\n",
        "    print(\"ChromaDB or model not available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this notebook, we've covered:\n",
        "\n",
        "1. **What Vector DBs are**: Specialized databases for efficient similarity search\n",
        "2. **FAISS**: Fast similarity search library with multiple indexing methods\n",
        "3. **ChromaDB**: Easy-to-use embedding database with built-in features\n",
        "4. **Practical Applications**: RAG systems, semantic search, and more\n",
        "\n",
        "### Key Takeaways:\n",
        "\n",
        "- Vector DBs are essential for scaling similarity search beyond thousands of vectors\n",
        "- FAISS provides maximum performance and flexibility\n",
        "- ChromaDB offers ease of use and production-ready features\n",
        "- Both are valuable tools depending on your use case\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "- Explore other vector DBs (Pinecone, Weaviate, Qdrant)\n",
        "- Build a complete RAG system with LLM integration\n",
        "- Experiment with different embedding models\n",
        "- Scale to larger datasets and measure performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample = [\n",
        "  {\n",
        "    \"name\": \"John Doe\",\n",
        "    \"emails\": [\"john.doe@example.com\", \"j.doe@work.com\"]\n",
        "  },\n",
        "  {\n",
        "    \"name\": \"Jane Smith\",\n",
        "    \"emails\": [\"jane.smith@example.com\", \"jsmith@business.org\", \"jane@home.net\"]\n",
        "  },\n",
        "  {\n",
        "    \"name\": \"Alice Williams\",\n",
        "    \"emails\": [\"alice.w@mail.com\", \"family.w@shared.com\"]\n",
        "  },\n",
        "  {\n",
        "    \"name\": \"Bob Williams\",\n",
        "    \"emails\": [\"bob@personal.io\", \"family.w@shared.com\"]\n",
        "  },\n",
        "  {\n",
        "    \"name\": \"Charlie Williams\",\n",
        "    \"emails\": [\"charlie.w@web.com\", \"family.w@shared.com\"]\n",
        "  },\n",
        "  {\n",
        "    \"name\": \"David Brown\",\n",
        "    \"emails\": [\"david.brown@unique.com\", \"d.brown@university.edu\"]\n",
        "  },\n",
        "  {\n",
        "    \"name\": \"Johnny Doe\",\n",
        "    \"emails\": [\"john.doe@example.com\", \"johnny@gmail.com\"]\n",
        "  },\n",
        "  {\n",
        "    \"name\": \"J. Smith\",\n",
        "    \"emails\": [\"jsmith@business.org\", \"jane.smith@example.com\", \"jsmith@business.org\"]\n",
        "  },\n",
        "  {\n",
        "    \"name\": \"Emily Davis\",\n",
        "    \"emails\": [\"emily.d@fastmail.com\"]\n",
        "  },\n",
        "  {\n",
        "    \"name\": \"Frank Miller\",\n",
        "    \"emails\": []\n",
        "  }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding Union-Find (Disjoint Set Union)\n",
        "\n",
        "**Union-Find** (also called Disjoint Set Union) is a data structure that efficiently tracks groups of connected elements. It's perfect for finding connected components in graphs.\n",
        "\n",
        "### The Problem It Solves\n",
        "\n",
        "Imagine you have multiple items, and you want to know which items are \"connected\" to each other. In our case:\n",
        "- Accounts are connected if they share an email\n",
        "- We want to find all groups of connected accounts\n",
        "\n",
        "### How It Works\n",
        "\n",
        "Union-Find uses two main operations:\n",
        "\n",
        "1. **`find(x)`**: Finds the \"representative\" (root) of the group containing `x`\n",
        "2. **`union(x, y)`**: Merges the groups containing `x` and `y` into one group\n",
        "\n",
        "### Visual Example\n",
        "\n",
        "Let's say we have 5 accounts (0-4):\n",
        "- Account 0 and 1 share an email → connect them\n",
        "- Account 2 and 3 share an email → connect them  \n",
        "- Account 1 and 2 share an email → connect them\n",
        "\n",
        "```\n",
        "Initially: [0] [1] [2] [3] [4]  (5 separate groups)\n",
        "\n",
        "After union(0, 1): [0-1] [2] [3] [4]  (4 groups)\n",
        "\n",
        "After union(2, 3): [0-1] [2-3] [4]  (3 groups)\n",
        "\n",
        "After union(1, 2): [0-1-2-3] [4]  (2 groups - all connected!)\n",
        "```\n",
        "\n",
        "### Implementation Details\n",
        "\n",
        "- **Parent Array**: Each element points to its parent. The root points to itself.\n",
        "- **Path Compression**: When finding the root, we update all nodes along the path to point directly to the root (makes future lookups faster)\n",
        "- **Union by Rank**: (Optional optimization) Always attach smaller tree to larger tree\n",
        "\n",
        "### Why Use It Here?\n",
        "\n",
        "For account merging:\n",
        "- If Account A shares email with Account B, and Account B shares email with Account C\n",
        "- Then A, B, and C should all be in the same merged group\n",
        "- Union-Find automatically handles these transitive connections!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step-by-step Union-Find demonstration\n",
        "# Let's trace through a simple example\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"Union-Find Step-by-Step Example\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Simple example: 4 accounts\n",
        "accounts_simple = [\n",
        "    {\"name\": \"A\", \"emails\": [\"email1\"]},\n",
        "    {\"name\": \"B\", \"emails\": [\"email1\", \"email2\"]},  # Shares email1 with A\n",
        "    {\"name\": \"C\", \"emails\": [\"email2\", \"email3\"]},  # Shares email2 with B\n",
        "    {\"name\": \"D\", \"emails\": [\"email4\"]}  # Isolated\n",
        "]\n",
        "\n",
        "n = len(accounts_simple)\n",
        "parent = list(range(n))  # Initially, each account is its own parent\n",
        "\n",
        "def find(x):\n",
        "    \"\"\"Find the root of x with path compression\"\"\"\n",
        "    if parent[x] != x:\n",
        "        parent[x] = find(parent[x])  # Path compression\n",
        "    return parent[x]\n",
        "\n",
        "def union(x, y):\n",
        "    \"\"\"Union the groups containing x and y\"\"\"\n",
        "    root_x = find(x)\n",
        "    root_y = find(y)\n",
        "    if root_x != root_y:\n",
        "        parent[root_y] = root_x\n",
        "        return True  # Actually merged\n",
        "    return False  # Already in same group\n",
        "\n",
        "print(f\"\\nInitial state: {parent}\")\n",
        "print(\"Each account is its own parent (separate groups)\\n\")\n",
        "\n",
        "# Build email mapping\n",
        "from collections import defaultdict\n",
        "email_to_accounts = defaultdict(set)\n",
        "for idx, account in enumerate(accounts_simple):\n",
        "    for email in account[\"emails\"]:\n",
        "        email_to_accounts[email].add(idx)\n",
        "\n",
        "print(\"Email to accounts mapping:\")\n",
        "for email, indices in email_to_accounts.items():\n",
        "    print(f\"  {email}: accounts {sorted(indices)}\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 70)\n",
        "print(\"Performing unions for accounts sharing emails:\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "step = 1\n",
        "for email, account_indices in email_to_accounts.items():\n",
        "    account_list = list(account_indices)\n",
        "    if len(account_list) > 1:\n",
        "        print(f\"\\nStep {step}: Email '{email}' connects accounts {account_list}\")\n",
        "        for i in range(1, len(account_list)):\n",
        "            merged = union(account_list[0], account_list[i])\n",
        "            if merged:\n",
        "                print(f\"  → Union({account_list[0]}, {account_list[i]})\")\n",
        "                print(f\"  → Parent array: {parent}\")\n",
        "                print(f\"  → Groups: \", end=\"\")\n",
        "                # Show current groups\n",
        "                groups = defaultdict(list)\n",
        "                for idx in range(n):\n",
        "                    groups[find(idx)].append(idx)\n",
        "                print([sorted(groups[g]) for g in groups])\n",
        "        step += 1\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Final Result:\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Parent array: {parent}\")\n",
        "\n",
        "# Show final groups\n",
        "final_groups = defaultdict(list)\n",
        "for idx in range(n):\n",
        "    root = find(idx)\n",
        "    final_groups[root].append(idx)\n",
        "\n",
        "print(\"\\nMerged groups:\")\n",
        "for root, indices in final_groups.items():\n",
        "    names = [accounts_simple[i][\"name\"] for i in indices]\n",
        "    emails = set()\n",
        "    for i in indices:\n",
        "        emails.update(accounts_simple[i][\"emails\"])\n",
        "    print(f\"  Group {root}: {names} → {sorted(emails)}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Key Insight:\")\n",
        "print(\"=\" * 70)\n",
        "print(\"Account A and B share email1, B and C share email2.\")\n",
        "print(\"Even though A and C don't directly share an email,\")\n",
        "print(\"Union-Find connects them through B! This is called 'transitive closure'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "defaultdict(<class 'set'>, {'john.doe@example.com': {0, 6}, 'j.doe@work.com': {0}, 'jane.smith@example.com': {1, 7}, 'jsmith@business.org': {1, 7}, 'jane@home.net': {1}, 'alice.w@mail.com': {2}, 'family.w@shared.com': {2, 3, 4}, 'bob@personal.io': {3}, 'charlie.w@web.com': {4}, 'david.brown@unique.com': {5}, 'd.brown@university.edu': {5}, 'johnny@gmail.com': {6}, 'emily.d@fastmail.com': {8}})\n",
            "Original accounts: 10\n",
            "Merged accounts: 6\n",
            "\n",
            "======================================================================\n",
            "Merged Accounts:\n",
            "======================================================================\n",
            "\n",
            "1. Names: ['John Doe', 'Johnny Doe']\n",
            "   Emails: ['j.doe@work.com', 'john.doe@example.com', 'johnny@gmail.com']\n",
            "\n",
            "2. Names: ['J. Smith', 'Jane Smith']\n",
            "   Emails: ['jane.smith@example.com', 'jane@home.net', 'jsmith@business.org']\n",
            "\n",
            "3. Names: ['Alice Williams', 'Bob Williams', 'Charlie Williams']\n",
            "   Emails: ['alice.w@mail.com', 'bob@personal.io', 'charlie.w@web.com', 'family.w@shared.com']\n",
            "\n",
            "4. Names: ['David Brown']\n",
            "   Emails: ['d.brown@university.edu', 'david.brown@unique.com']\n",
            "\n",
            "5. Names: ['Emily Davis']\n",
            "   Emails: ['emily.d@fastmail.com']\n",
            "\n",
            "6. Names: ['Frank Miller']\n",
            "   Emails: []\n"
          ]
        }
      ],
      "source": [
        "# Merge accounts that share the same email addresses\n",
        "# Using a graph-based approach to find connected components\n",
        "\n",
        "def merge_accounts_by_email(accounts):\n",
        "    \"\"\"\n",
        "    Merge accounts that share at least one email address.\n",
        "    Returns a list of merged accounts with names_used and emails.\n",
        "    \"\"\"\n",
        "    from collections import defaultdict\n",
        "    \n",
        "    # Build email-to-account mapping\n",
        "    email_to_accounts = defaultdict(set)\n",
        "    for idx, account in enumerate(accounts):\n",
        "        for email in account.get(\"emails\", []):\n",
        "            if email:  # Skip empty emails\n",
        "                email_to_accounts[email].add(idx)\n",
        "    \n",
        "    # Union-Find (Disjoint Set) to find connected components\n",
        "    parent = list(range(len(accounts)))\n",
        "    \n",
        "    def find(x):\n",
        "        if parent[x] != x:\n",
        "            parent[x] = find(parent[x])  # Path compression\n",
        "        return parent[x]\n",
        "    \n",
        "    def union(x, y):\n",
        "        root_x = find(x)\n",
        "        root_y = find(y)\n",
        "        if root_x != root_y:\n",
        "            parent[root_y] = root_x\n",
        "    \n",
        "    # Union accounts that share emails\n",
        "    for email, account_indices in email_to_accounts.items():\n",
        "        account_list = list(account_indices)\n",
        "        for i in range(1, len(account_list)):\n",
        "            union(account_list[0], account_list[i])\n",
        "    \n",
        "    # Group accounts by their root\n",
        "    merged_groups = defaultdict(lambda: {\"names_used\": [], \"emails\": set()})\n",
        "    \n",
        "    for idx, account in enumerate(accounts):\n",
        "        root = find(idx)\n",
        "        merged_groups[root][\"names_used\"].append(account[\"name\"])\n",
        "        # Add all emails from this account\n",
        "        for email in account.get(\"emails\", []):\n",
        "            if email:  # Skip empty emails\n",
        "                merged_groups[root][\"emails\"].add(email)\n",
        "    \n",
        "    # Convert to final format\n",
        "    merged_accounts = []\n",
        "    for group in merged_groups.values():\n",
        "        merged_accounts.append({\n",
        "            \"names_used\": sorted(group[\"names_used\"]),  # Sort for consistency\n",
        "            \"emails\": sorted(list(group[\"emails\"]))  # Sort for consistency\n",
        "        })\n",
        "    \n",
        "    return merged_accounts\n",
        "\n",
        "# Merge the sample accounts\n",
        "merged = merge_accounts_by_email(sample)\n",
        "\n",
        "print(\"Original accounts:\", len(sample))\n",
        "print(\"Merged accounts:\", len(merged))\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Merged Accounts:\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for i, account in enumerate(merged, 1):\n",
        "    print(f\"\\n{i}. Names: {account['names_used']}\")\n",
        "    print(f\"   Emails: {account['emails']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llm-learning",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
