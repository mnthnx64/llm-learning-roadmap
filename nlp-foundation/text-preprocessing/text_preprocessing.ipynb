{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Text Preprocessing\n",
        "\n",
        "This notebook covers essential text preprocessing techniques used in Natural Language Processing:\n",
        "- **Tokenization**: Breaking text into words, sentences, or subwords\n",
        "- **Stemming**: Reducing words to their root form\n",
        "- **Lemmatization**: Converting words to their base/dictionary form\n",
        "- **Named Entity Recognition (NER)**: Identifying entities like people, organizations, locations\n",
        "- **Part-of-Speech (POS) Tagging**: Identifying grammatical roles of words\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "- Understand different tokenization methods and when to use them\n",
        "- Apply stemming and lemmatization for text normalization\n",
        "- Use NER to extract structured information from text\n",
        "- Perform POS tagging for grammatical analysis\n",
        "- Compare and choose appropriate preprocessing techniques\n",
        "\n",
        "## Setup\n",
        "\n",
        "First, we need to install required libraries and download necessary data:\n",
        "- `nltk`: Natural Language Toolkit\n",
        "- `spacy`: Industrial-strength NLP library\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation\n",
        "\n",
        "Run this cell to install required packages (uncomment if needed):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install packages (uncomment if needed)\n",
        "# !pip install nltk spacy\n",
        "# !python -m spacy download en_core_web_sm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK data downloaded successfully!\n",
            "spaCy model loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize, wordpunct_tokenize\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag, ne_chunk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "# Download required NLTK data (run once)\n",
        "required_resources = [\n",
        "    'punkt',\n",
        "    'punkt_tab',  # Required for newer NLTK versions\n",
        "    'stopwords',\n",
        "    'wordnet',\n",
        "    'averaged_perceptron_tagger',\n",
        "    'maxent_ne_chunker',\n",
        "    'words'\n",
        "]\n",
        "\n",
        "try:\n",
        "    for resource in required_resources:\n",
        "        try:\n",
        "            nltk.download(resource, quiet=True)\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not download {resource}: {e}\")\n",
        "    \n",
        "    print(\"NLTK data downloaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading NLTK data: {e}\")\n",
        "    print(\"\\nIf you encounter issues, try running:\")\n",
        "    print(\"  nltk.download('averaged_perceptron_tagger')\")\n",
        "    print(\"or\")\n",
        "    print(\"  nltk.download('all')\")\n",
        "\n",
        "# Load spaCy model\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    print(\"spaCy model loaded successfully!\")\n",
        "except OSError:\n",
        "    print(\"spaCy model not found. Please run: python -m spacy download en_core_web_sm\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sample Text\n",
        "\n",
        "We'll use this sample text throughout the notebook to demonstrate various preprocessing techniques:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample text loaded!\n"
          ]
        }
      ],
      "source": [
        "# Sample text for demonstration\n",
        "sample_text = \"\"\"\n",
        "Natural Language Processing (NLP) is a branch of artificial intelligence that helps computers understand, \n",
        "interpret and manipulate human language. NLP draws from many disciplines, including computer science and \n",
        "computational linguistics, in its pursuit to fill the gap between human communication and computer understanding.\n",
        "\n",
        "Companies like Google, Microsoft, and OpenAI are leading the development of NLP technologies. \n",
        "These technologies are being used in chatbots, translation services, and virtual assistants.\n",
        "\n",
        "Dr. Sarah Johnson from Stanford University published a paper on transformer models in 2023. \n",
        "The research was conducted in California and involved collaboration with researchers from New York.\n",
        "\"\"\"\n",
        "print(\"Sample text loaded!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Tokenization\n",
        "\n",
        "Tokenization is the process of breaking down text into smaller units (tokens) such as words, sentences, or subwords. It's the first step in most NLP pipelines.\n",
        "\n",
        "## 1.1 Word Tokenization\n",
        "\n",
        "Word tokenization splits text into individual words.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Word Tokenization:\n",
            "['Natural', 'Language', 'Processing', 'is', 'amazing', '!', 'Let', \"'s\", 'learn', 'about', 'it', '.']\n",
            "\n",
            "Total tokens: 12\n"
          ]
        }
      ],
      "source": [
        "# Using NLTK's word_tokenize\n",
        "text = \"Natural Language Processing is amazing! Let's learn about it.\"\n",
        "words_nltk = word_tokenize(text)\n",
        "print(\"NLTK Word Tokenization:\")\n",
        "print(words_nltk)\n",
        "print(f\"\\nTotal tokens: {len(words_nltk)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "spaCy Word Tokenization:\n",
            "['Natural', 'Language', 'Processing', 'is', 'amazing', '!', 'Let', \"'s\", 'learn', 'about', 'it', '.']\n",
            "\n",
            "Total tokens: 12\n"
          ]
        }
      ],
      "source": [
        "# Using spaCy\n",
        "doc = nlp(text)\n",
        "words_spacy = [token.text for token in doc]\n",
        "print(\"spaCy Word Tokenization:\")\n",
        "print(words_spacy)\n",
        "print(f\"\\nTotal tokens: {len(words_spacy)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Regex Word Tokenization:\n",
            "['Natural', 'Language', 'Processing', 'is', 'amazing', 'Let', 's', 'learn', 'about', 'it']\n",
            "\n",
            "Total tokens: 10\n"
          ]
        }
      ],
      "source": [
        "# Using simple regex (for comparison)\n",
        "words_regex = re.findall(r'\\b\\w+\\b', text)\n",
        "print(\"Regex Word Tokenization:\")\n",
        "print(words_regex)\n",
        "print(f\"\\nTotal tokens: {len(words_regex)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 Sentence Tokenization\n",
        "\n",
        "Sentence tokenization splits text into sentences.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Sentence Tokenization:\n",
            "\n",
            "Sentence 1: \n",
            "Natural Language Processing (NLP) is a branch of artificial intelligence that h...\n",
            "\n",
            "Sentence 2: NLP draws from many disciplines, including computer science and \n",
            "computational l...\n",
            "\n",
            "Sentence 3: Companies like Google, Microsoft, and OpenAI are leading the development of NLP ...\n",
            "\n",
            "Sentence 4: These technologies are being used in chatbots, translation services, and virtual...\n",
            "\n",
            "Sentence 5: Dr. Sarah Johnson from Stanford University published a paper on transformer mode...\n",
            "\n",
            "Sentence 6: The research was conducted in California and involved collaboration with researc...\n",
            "\n",
            "Total sentences: 6\n"
          ]
        }
      ],
      "source": [
        "# Using NLTK's sent_tokenize\n",
        "sentences_nltk = sent_tokenize(sample_text)\n",
        "print(\"NLTK Sentence Tokenization:\")\n",
        "for i, sent in enumerate(sentences_nltk, 1):\n",
        "    print(f\"\\nSentence {i}: {sent[:80]}...\")\n",
        "print(f\"\\nTotal sentences: {len(sentences_nltk)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "spaCy Sentence Tokenization:\n",
            "\n",
            "Sentence 1: \n",
            "Natural Language Processing (NLP) is a branch of artificial intelligence that h...\n",
            "\n",
            "Sentence 2: NLP draws from many disciplines, including computer science and \n",
            "computational l...\n",
            "\n",
            "Sentence 3: Companies like Google, Microsoft, and OpenAI are leading the development of NLP ...\n",
            "\n",
            "Sentence 4: These technologies are being used in chatbots, translation services, and virtual...\n",
            "\n",
            "Sentence 5: Dr. Sarah Johnson from Stanford University published a paper on transformer mode...\n",
            "\n",
            "Sentence 6: The research was conducted in California and involved collaboration with researc...\n",
            "\n",
            "Total sentences: 6\n"
          ]
        }
      ],
      "source": [
        "# Using spaCy\n",
        "doc = nlp(sample_text)\n",
        "sentences_spacy = [sent.text for sent in doc.sents]\n",
        "print(\"spaCy Sentence Tokenization:\")\n",
        "for i, sent in enumerate(sentences_spacy, 1):\n",
        "    print(f\"\\nSentence {i}: {sent[:80]}...\")\n",
        "print(f\"\\nTotal sentences: {len(sentences_spacy)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token Analysis:\n",
            "Token           Text            Lemma           POS        Is Alpha  \n",
            "----------------------------------------------------------------------\n",
            "I               I               I               PRON       1         \n",
            "'m              'm              be              AUX        0         \n",
            "learning        learning        learn           VERB       1         \n",
            "NLP             NLP             NLP             PROPN      1         \n",
            "!               !               !               PUNCT      0         \n",
            "It              It              it              PRON       1         \n",
            "'s              's              be              AUX        0         \n",
            "amazing         amazing         amazing         ADJ        1         \n",
            ".               .               .               PUNCT      0         \n"
          ]
        }
      ],
      "source": [
        "# spaCy token attributes\n",
        "text_example = \"I'm learning NLP! It's amazing.\"\n",
        "doc = nlp(text_example)\n",
        "\n",
        "print(\"Token Analysis:\")\n",
        "print(f\"{'Token':<15} {'Text':<15} {'Lemma':<15} {'POS':<10} {'Is Alpha':<10}\")\n",
        "print(\"-\" * 70)\n",
        "for token in doc:\n",
        "    print(f\"{str(token):<15} {token.text:<15} {token.lemma_:<15} {token.pos_:<10} {token.is_alpha:<10}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Stemming\n",
        "\n",
        "Stemming reduces words to their root form by removing suffixes. It's a rule-based approach that may not always produce valid words.\n",
        "\n",
        "**Example**: \"running\" → \"run\", \"happier\" → \"happi\"\n",
        "\n",
        "## 2.1 Porter Stemmer\n",
        "\n",
        "The Porter Stemmer is one of the most common stemming algorithms.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Porter Stemmer Results:\n",
            "Original        Stemmed        \n",
            "------------------------------\n",
            "running         run            \n",
            "runs            run            \n",
            "ran             ran            \n",
            "happier         happier        \n",
            "happiest        happiest       \n",
            "happiness       happi          \n",
            "studies         studi          \n",
            "studying        studi          \n",
            "studied         studi          \n",
            "flies           fli            \n",
            "flying          fli            \n",
            "flew            flew           \n"
          ]
        }
      ],
      "source": [
        "# Initialize Porter Stemmer\n",
        "porter = PorterStemmer()\n",
        "\n",
        "# Example words\n",
        "words = [\"running\", \"runs\", \"ran\", \"happier\", \"happiest\", \"happiness\", \n",
        "         \"studies\", \"studying\", \"studied\", \"flies\", \"flying\", \"flew\"]\n",
        "\n",
        "print(\"Porter Stemmer Results:\")\n",
        "print(f\"{'Original':<15} {'Stemmed':<15}\")\n",
        "print(\"-\" * 30)\n",
        "for word in words:\n",
        "    stemmed = porter.stem(word)\n",
        "    print(f\"{word:<15} {stemmed:<15}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original words (first 20):\n",
            "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'branch', 'of', 'artificial', 'intelligence', 'that', 'helps', 'computers', 'understand', ',', 'interpret', 'and', 'manipulate']\n",
            "\n",
            "Stemmed words (first 20):\n",
            "['natur', 'languag', 'process', 'nlp', 'is', 'a', 'branch', 'of', 'artifici', 'intellig', 'that', 'help', 'comput', 'understand', 'interpret', 'and', 'manipul', 'human', 'languag', 'nlp']\n"
          ]
        }
      ],
      "source": [
        "# Stemming on sample text\n",
        "words = word_tokenize(sample_text)\n",
        "stemmed_words = [porter.stem(word) for word in words if word.isalpha()]\n",
        "\n",
        "print(\"Original words (first 20):\")\n",
        "print(words[:20])\n",
        "print(\"\\nStemmed words (first 20):\")\n",
        "print(stemmed_words[:20])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 Snowball Stemmer\n",
        "\n",
        "The Snowball Stemmer (also known as Porter2) is an improved version that supports multiple languages.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Snowball Stemmer Results:\n",
            "Original        Stemmed        \n",
            "------------------------------\n",
            "running         run            \n",
            "runs            run            \n",
            "ran             ran            \n",
            "happier         happier        \n",
            "happiest        happiest       \n",
            "happiness       happi          \n",
            "studies         studi          \n",
            "studying        studi          \n",
            "studied         studi          \n"
          ]
        }
      ],
      "source": [
        "# Initialize Snowball Stemmer (for English)\n",
        "snowball = SnowballStemmer(language='english')\n",
        "\n",
        "words = [\"running\", \"runs\", \"ran\", \"happier\", \"happiest\", \"happiness\", \n",
        "         \"studies\", \"studying\", \"studied\"]\n",
        "\n",
        "print(\"Snowball Stemmer Results:\")\n",
        "print(f\"{'Original':<15} {'Stemmed':<15}\")\n",
        "print(\"-\" * 30)\n",
        "for word in words:\n",
        "    stemmed = snowball.stem(word)\n",
        "    print(f\"{word:<15} {stemmed:<15}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Lemmatization\n",
        "\n",
        "Lemmatization converts words to their base or dictionary form (lemma). Unlike stemming, lemmatization considers the context and part of speech, producing valid words.\n",
        "\n",
        "**Example**: \"running\" → \"run\", \"better\" → \"well\", \"was\" → \"be\"\n",
        "\n",
        "## 3.1 NLTK WordNet Lemmatizer\n",
        "\n",
        "The WordNetLemmatizer uses the WordNet database to find the correct lemma based on the word's part of speech.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK WordNet Lemmatizer Results:\n",
            "Original        Lemmatized     \n",
            "------------------------------\n",
            "running         running        \n",
            "runs            run            \n",
            "ran             ran            \n",
            "happier         happier        \n",
            "happiest        happiest       \n",
            "happiness       happiness      \n",
            "studies         study          \n",
            "studying        studying       \n",
            "studied         studied        \n",
            "was             wa             \n",
            "better          better         \n",
            "mice            mouse          \n",
            "\n",
            "==================================================\n",
            "With POS tags (more accurate):\n",
            "Original        POS        Lemmatized     \n",
            "----------------------------------------\n",
            "running         v          run            \n",
            "better          a          good           \n",
            "studies         n          study          \n",
            "was             v          be             \n"
          ]
        }
      ],
      "source": [
        "# Initialize WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Example words\n",
        "words = [\"running\", \"runs\", \"ran\", \"happier\", \"happiest\", \"happiness\", \n",
        "         \"studies\", \"studying\", \"studied\", \"was\", \"better\", \"mice\"]\n",
        "\n",
        "print(\"NLTK WordNet Lemmatizer Results:\")\n",
        "print(f\"{'Original':<15} {'Lemmatized':<15}\")\n",
        "print(\"-\" * 30)\n",
        "for word in words:\n",
        "    # For better results, specify POS tag (verb='v', noun='n', adjective='a', adverb='r')\n",
        "    # Default assumes noun\n",
        "    lemmatized = lemmatizer.lemmatize(word)\n",
        "    print(f\"{word:<15} {lemmatized:<15}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"With POS tags (more accurate):\")\n",
        "print(f\"{'Original':<15} {'POS':<10} {'Lemmatized':<15}\")\n",
        "print(\"-\" * 40)\n",
        "test_words = [(\"running\", \"v\"), (\"better\", \"a\"), (\"studies\", \"n\"), (\"was\", \"v\")]\n",
        "for word, pos in test_words:\n",
        "    lemmatized = lemmatizer.lemmatize(word, pos=pos)\n",
        "    print(f\"{word:<15} {pos:<10} {lemmatized:<15}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "spaCy Lemmatization:\n",
            "Text            Lemma           POS       \n",
            "----------------------------------------\n",
            "I               I               PRON      \n",
            "was             be              AUX       \n",
            "running         run             VERB      \n",
            "faster          fast            ADV       \n",
            "and             and             CCONJ     \n",
            "studying        study           VERB      \n",
            "harder          hard            ADV       \n",
            "The             the             DET       \n",
            "studies         study           NOUN      \n",
            "were            be              AUX       \n",
            "better          well            ADJ       \n"
          ]
        }
      ],
      "source": [
        "# Using spaCy for lemmatization\n",
        "text = \"I was running faster and studying harder. The studies were better.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "print(\"spaCy Lemmatization:\")\n",
        "print(f\"{'Text':<15} {'Lemma':<15} {'POS':<10}\")\n",
        "print(\"-\" * 40)\n",
        "for token in doc:\n",
        "    if not token.is_punct and not token.is_space:\n",
        "        print(f\"{token.text:<15} {token.lemma_:<15} {token.pos_:<10}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "spaCy Lemmatization:\n",
            "Text            Lemma           POS       \n",
            "----------------------------------------\n",
            "I               I               PRON      \n",
            "was             be              AUX       \n",
            "running         run             VERB      \n",
            "faster          fast            ADV       \n",
            "and             and             CCONJ     \n",
            "studying        study           VERB      \n",
            "harder          hard            ADV       \n",
            "The             the             DET       \n",
            "studies         study           NOUN      \n",
            "were            be              AUX       \n",
            "better          well            ADJ       \n"
          ]
        }
      ],
      "source": [
        "\n",
        "text = \"I was running faster and studying harder. The studies were better.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "print(\"spaCy Lemmatization:\")\n",
        "print(f\"{'Text':<15} {'Lemma':<15} {'POS':<10}\")\n",
        "print(\"-\" * 40)\n",
        "for token in doc:\n",
        "    if not token.is_punct and not token.is_space:\n",
        "        print(f\"{token.text:<15} {token.lemma_:<15} {token.pos_:<10}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stemming vs Lemmatization Comparison:\n",
            "Word            Stemmed         Lemmatized     \n",
            "---------------------------------------------\n",
            "running         run             running        \n",
            "happier         happier         happier        \n",
            "studies         studi           study          \n",
            "was             wa              wa             \n",
            "better          better          better         \n"
          ]
        }
      ],
      "source": [
        "# Compare Stemming vs Lemmatization\n",
        "# Initialize WordNetLemmatizer (if not already done)\n",
        "if 'lemmatizer' not in globals():\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "comparison_words = [\"running\", \"happier\", \"studies\", \"was\", \"better\"]\n",
        "\n",
        "print(\"Stemming vs Lemmatization Comparison:\")\n",
        "print(f\"{'Word':<15} {'Stemmed':<15} {'Lemmatized':<15}\")\n",
        "print(\"-\" * 45)\n",
        "for word in comparison_words:\n",
        "    stemmed = porter.stem(word)\n",
        "    lemmatized = lemmatizer.lemmatize(word)\n",
        "    print(f\"{word:<15} {stemmed:<15} {lemmatized:<15}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /Users/mx98/nltk_data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK POS Tagging:\n",
            "Word                 POS Tag   \n",
            "------------------------------\n",
            "Natural              JJ        \n",
            "Language             NNP       \n",
            "Processing           NNP       \n",
            "is                   VBZ       \n",
            "amazing              JJ        \n",
            "and                  CC        \n",
            "helps                VBZ       \n",
            "computers            NNS       \n",
            "understand           JJ        \n",
            "text                 NN        \n",
            ".                    .         \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        }
      ],
      "source": [
        "# Using NLTK POS tagging\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "text = \"Natural Language Processing is amazing and helps computers understand text.\"\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "try:\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    print(\"NLTK POS Tagging:\")\n",
        "    print(f\"{'Word':<20} {'POS Tag':<10}\")\n",
        "    print(\"-\" * 30)\n",
        "    for word, tag in pos_tags:\n",
        "        print(f\"{word:<20} {tag:<10}\")\n",
        "except LookupError as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"\\nPOS tagger not found. Downloading...\")\n",
        "    nltk.download('averaged_perceptron_tagger', quiet=False)\n",
        "    print(\"\\nPlease re-run this cell after downloading.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "spaCy POS Tagging:\n",
            "Word                 POS        Tag        Description                   \n",
            "----------------------------------------------------------------------\n",
            "Natural              PROPN      NNP        proper noun                   \n",
            "Language             PROPN      NNP        proper noun                   \n",
            "Processing           NOUN       NN         noun                          \n",
            "is                   AUX        VBZ        auxiliary                     \n",
            "amazing              ADJ        JJ         adjective                     \n",
            "and                  CCONJ      CC         coordinating conjunction      \n",
            "helps                VERB       VBZ        verb                          \n",
            "computers            NOUN       NNS        noun                          \n",
            "understand           VERB       VB         verb                          \n",
            "text                 NOUN       NN         noun                          \n"
          ]
        }
      ],
      "source": [
        "# Using spaCy POS tagging\n",
        "doc = nlp(text)\n",
        "\n",
        "print(\"spaCy POS Tagging:\")\n",
        "print(f\"{'Word':<20} {'POS':<10} {'Tag':<10} {'Description':<30}\")\n",
        "print(\"-\" * 70)\n",
        "for token in doc:\n",
        "    if not token.is_punct and not token.is_space:\n",
        "        print(f\"{token.text:<20} {token.pos_:<10} {token.tag_:<10} {spacy.explain(token.pos_):<30}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. Named Entity Recognition (NER)\n",
        "\n",
        "NER identifies and classifies named entities in text (people, organizations, locations, dates, etc.).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "spaCy Named Entity Recognition:\n",
            "Entity                    Label           Description                   \n",
            "----------------------------------------------------------------------\n",
            "NLP                       ORG             Companies, agencies, institutions, etc.\n",
            "NLP                       ORG             Companies, agencies, institutions, etc.\n",
            "Google                    ORG             Companies, agencies, institutions, etc.\n",
            "Microsoft                 ORG             Companies, agencies, institutions, etc.\n",
            "OpenAI                    GPE             Countries, cities, states     \n",
            "NLP                       ORG             Companies, agencies, institutions, etc.\n",
            "Sarah Johnson             PERSON          People, including fictional   \n",
            "Stanford University       ORG             Companies, agencies, institutions, etc.\n",
            "2023                      DATE            Absolute or relative dates or periods\n",
            "California                GPE             Countries, cities, states     \n",
            "New York                  GPE             Countries, cities, states     \n"
          ]
        }
      ],
      "source": [
        "# Using spaCy for NER\n",
        "doc = nlp(sample_text)\n",
        "\n",
        "print(\"spaCy Named Entity Recognition:\")\n",
        "print(f\"{'Entity':<25} {'Label':<15} {'Description':<30}\")\n",
        "print(\"-\" * 70)\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.text:<25} {ent.label_:<15} {spacy.explain(ent.label_):<30}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entity visualization (HTML):\n",
            "Note: To view the visualization, save the HTML to a file or use a web browser.\n",
            "\n",
            "HTML preview (first 500 chars):\n",
            "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\"><br>Natural Language Processing (\n",
            "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
            "    NLP\n",
            "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
            "</mark>\n",
            ") is a branch of artificial intelligence that helps computers understand, <br>interpret and manipulate huma...\n"
          ]
        }
      ],
      "source": [
        "# Visualize NER with spaCy's displaCy (if available)\n",
        "# This creates an HTML visualization\n",
        "from spacy import displacy\n",
        "\n",
        "# Create a visualization of entities\n",
        "try:\n",
        "    # Try to render in Jupyter notebook\n",
        "    html = displacy.render(doc, style=\"ent\", jupyter=True)\n",
        "except ImportError:\n",
        "    # If IPython display is not available, render as HTML string\n",
        "    try:\n",
        "        html = displacy.render(doc, style=\"ent\", jupyter=False)\n",
        "        print(\"Entity visualization (HTML):\")\n",
        "        print(\"Note: To view the visualization, save the HTML to a file or use a web browser.\")\n",
        "        print(f\"\\nHTML preview (first 500 chars):\\n{html[:500]}...\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not generate visualization: {e}\")\n",
        "        print(\"\\nYou can still see the entities in the previous cell output.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error generating visualization: {e}\")\n",
        "    print(\"\\nYou can still see the entities in the previous cell output.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entities by Type:\n",
            "\n",
            "ORG (Companies, agencies, institutions, etc.):\n",
            "  NLP, Microsoft, Google, Stanford University\n",
            "\n",
            "GPE (Countries, cities, states):\n",
            "  California, New York, OpenAI\n",
            "\n",
            "PERSON (People, including fictional):\n",
            "  Sarah Johnson\n",
            "\n",
            "DATE (Absolute or relative dates or periods):\n",
            "  2023\n"
          ]
        }
      ],
      "source": [
        "# Extract entities by type\n",
        "print(\"Entities by Type:\")\n",
        "entities_by_type = {}\n",
        "for ent in doc.ents:\n",
        "    if ent.label_ not in entities_by_type:\n",
        "        entities_by_type[ent.label_] = []\n",
        "    entities_by_type[ent.label_].append(ent.text)\n",
        "\n",
        "for label, entities in entities_by_type.items():\n",
        "    print(f\"\\n{label} ({spacy.explain(label)}):\")\n",
        "    print(f\"  {', '.join(set(entities))}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook covered:\n",
        "- ✅ **Tokenization**: Word and sentence tokenization using NLTK and spaCy\n",
        "- ✅ **Stemming**: Porter and Snowball stemmers for reducing words to root forms\n",
        "- ✅ **Lemmatization**: Converting words to dictionary forms with POS awareness\n",
        "- ✅ **POS Tagging**: Identifying grammatical roles of words\n",
        "- ✅ **NER**: Extracting named entities (people, organizations, locations, etc.)\n",
        "\n",
        "### When to Use What?\n",
        "\n",
        "- **Stemming**: Fast, good for search engines, information retrieval\n",
        "- **Lemmatization**: Better for tasks requiring valid words, more accurate but slower\n",
        "- **POS Tagging**: Needed for syntax analysis, dependency parsing, better lemmatization\n",
        "- **NER**: Extract structured information, build knowledge graphs, information extraction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llm-learning",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
