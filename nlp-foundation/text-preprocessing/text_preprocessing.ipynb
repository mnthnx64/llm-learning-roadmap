{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Text Preprocessing\n",
        "\n",
        "This notebook covers essential text preprocessing techniques used in Natural Language Processing:\n",
        "- **Tokenization**: Breaking text into words, sentences, or subwords\n",
        "- **Stemming**: Reducing words to their root form\n",
        "- **Lemmatization**: Converting words to their base/dictionary form\n",
        "- **Named Entity Recognition (NER)**: Identifying entities like people, organizations, locations\n",
        "- **Part-of-Speech (POS) Tagging**: Identifying grammatical roles of words\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "- Understand different tokenization methods and when to use them\n",
        "- Apply stemming and lemmatization for text normalization\n",
        "- Use NER to extract structured information from text\n",
        "- Perform POS tagging for grammatical analysis\n",
        "- Compare and choose appropriate preprocessing techniques\n",
        "\n",
        "## Setup\n",
        "\n",
        "First, we need to install required libraries and download necessary data:\n",
        "- `nltk`: Natural Language Toolkit\n",
        "- `spacy`: Industrial-strength NLP library\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation\n",
        "\n",
        "Run this cell to install required packages (uncomment if needed):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install packages (uncomment if needed)\n",
        "# !pip install nltk spacy\n",
        "# !python -m spacy download en_core_web_sm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize, wordpunct_tokenize\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag, ne_chunk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "# Download required NLTK data (run once)\n",
        "try:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "    nltk.download('wordnet', quiet=True)\n",
        "    nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "    nltk.download('maxent_ne_chunker', quiet=True)\n",
        "    nltk.download('words', quiet=True)\n",
        "    print(\"NLTK data downloaded successfully!\")\n",
        "except:\n",
        "    print(\"Some NLTK data may already be downloaded\")\n",
        "\n",
        "# Load spaCy model\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    print(\"spaCy model loaded successfully!\")\n",
        "except OSError:\n",
        "    print(\"spaCy model not found. Please run: python -m spacy download en_core_web_sm\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sample Text\n",
        "\n",
        "We'll use this sample text throughout the notebook to demonstrate various preprocessing techniques:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample text for demonstration\n",
        "sample_text = \"\"\"\n",
        "Natural Language Processing (NLP) is a branch of artificial intelligence that helps computers understand, \n",
        "interpret and manipulate human language. NLP draws from many disciplines, including computer science and \n",
        "computational linguistics, in its pursuit to fill the gap between human communication and computer understanding.\n",
        "\n",
        "Companies like Google, Microsoft, and OpenAI are leading the development of NLP technologies. \n",
        "These technologies are being used in chatbots, translation services, and virtual assistants.\n",
        "\n",
        "Dr. Sarah Johnson from Stanford University published a paper on transformer models in 2023. \n",
        "The research was conducted in California and involved collaboration with researchers from New York.\n",
        "\"\"\"\n",
        "print(\"Sample text loaded!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Tokenization\n",
        "\n",
        "Tokenization is the process of breaking down text into smaller units (tokens) such as words, sentences, or subwords. It's the first step in most NLP pipelines.\n",
        "\n",
        "## 1.1 Word Tokenization\n",
        "\n",
        "Word tokenization splits text into individual words.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using NLTK's word_tokenize\n",
        "text = \"Natural Language Processing is amazing! Let's learn about it.\"\n",
        "words_nltk = word_tokenize(text)\n",
        "print(\"NLTK Word Tokenization:\")\n",
        "print(words_nltk)\n",
        "print(f\"\\nTotal tokens: {len(words_nltk)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using spaCy\n",
        "doc = nlp(text)\n",
        "words_spacy = [token.text for token in doc]\n",
        "print(\"spaCy Word Tokenization:\")\n",
        "print(words_spacy)\n",
        "print(f\"\\nTotal tokens: {len(words_spacy)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using simple regex (for comparison)\n",
        "words_regex = re.findall(r'\\b\\w+\\b', text)\n",
        "print(\"Regex Word Tokenization:\")\n",
        "print(words_regex)\n",
        "print(f\"\\nTotal tokens: {len(words_regex)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 Sentence Tokenization\n",
        "\n",
        "Sentence tokenization splits text into sentences.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using NLTK's sent_tokenize\n",
        "sentences_nltk = sent_tokenize(sample_text)\n",
        "print(\"NLTK Sentence Tokenization:\")\n",
        "for i, sent in enumerate(sentences_nltk, 1):\n",
        "    print(f\"\\nSentence {i}: {sent[:80]}...\")\n",
        "print(f\"\\nTotal sentences: {len(sentences_nltk)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using spaCy\n",
        "doc = nlp(sample_text)\n",
        "sentences_spacy = [sent.text for sent in doc.sents]\n",
        "print(\"spaCy Sentence Tokenization:\")\n",
        "for i, sent in enumerate(sentences_spacy, 1):\n",
        "    print(f\"\\nSentence {i}: {sent[:80]}...\")\n",
        "print(f\"\\nTotal sentences: {len(sentences_spacy)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "think"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# spaCy token attributes\n",
        "text_example = \"I'm learning NLP! It's amazing.\"\n",
        "doc = nlp(text_example)\n",
        "\n",
        "print(\"Token Analysis:\")\n",
        "print(f\"{'Token':<15} {'Text':<15} {'Lemma':<15} {'POS':<10} {'Is Alpha':<10}\")\n",
        "print(\"-\" * 70)\n",
        "for token in doc:\n",
        "    print(f\"{str(token):<15} {token.text:<15} {token.lemma_:<15} {token.pos_:<10} {token.is_alpha:<10}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Stemming\n",
        "\n",
        "Stemming reduces words to their root form by removing suffixes. It's a rule-based approach that may not always produce valid words.\n",
        "\n",
        "**Example**: \"running\" → \"run\", \"happier\" → \"happi\"\n",
        "\n",
        "## 2.1 Porter Stemmer\n",
        "\n",
        "The Porter Stemmer is one of the most common stemming algorithms.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Porter Stemmer\n",
        "porter = PorterStemmer()\n",
        "\n",
        "# Example words\n",
        "words = [\"running\", \"runs\", \"ran\", \"happier\", \"happiest\", \"happiness\", \n",
        "         \"studies\", \"studying\", \"studied\", \"flies\", \"flying\", \"flew\"]\n",
        "\n",
        "print(\"Porter Stemmer Results:\")\n",
        "print(f\"{'Original':<15} {'Stemmed':<15}\")\n",
        "print(\"-\" * 30)\n",
        "for word in words:\n",
        "    stemmed = porter.stem(word)\n",
        "    print(f\"{word:<15} {stemmed:<15}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stemming on sample text\n",
        "words = word_tokenize(sample_text)\n",
        "stemmed_words = [porter.stem(word) for word in words if word.isalpha()]\n",
        "\n",
        "print(\"Original words (first 20):\")\n",
        "print(words[:20])\n",
        "print(\"\\nStemmed words (first 20):\")\n",
        "print(stemmed_words[:20])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 Snowball Stemmer\n",
        "\n",
        "The Snowball Stemmer (also known as Porter2) is an improved version that supports multiple languages.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Snowball Stemmer (for English)\n",
        "snowball = SnowballStemmer(language='english')\n",
        "\n",
        "words = [\"running\", \"runs\", \"ran\", \"happier\", \"happiest\", \"happiness\", \n",
        "         \"studies\", \"studying\", \"studied\"]\n",
        "\n",
        "print(\"Snowball Stemmer Results:\")\n",
        "print(f\"{'Original':<15} {'Stemmed':<15}\")\n",
        "print(\"-\" * 30)\n",
        "for word in words:\n",
        "    stemmed = snowball.stem(word)\n",
        "    print(f\"{word:<15} {stemmed:<15}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using spaCy for lemmatization\n",
        "text = \"I was running faster and studying harder. The studies were better.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "print(\"spaCy Lemmatization:\")\n",
        "print(f\"{'Text':<15} {'Lemma':<15} {'POS':<10}\")\n",
        "print(\"-\" * 40)\n",
        "for token in doc:\n",
        "    if not token.is_punct and not token.is_space:\n",
        "        print(f\"{token.text:<15} {token.lemma_:<15} {token.pos_:<10}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.3 Stemming vs Lemmatization Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare Stemming vs Lemmatization\n",
        "comparison_words = [\"running\", \"happier\", \"studies\", \"was\", \"better\"]\n",
        "\n",
        "print(\"Stemming vs Lemmatization Comparison:\")\n",
        "print(f\"{'Word':<15} {'Stemmed':<15} {'Lemmatized':<15}\")\n",
        "print(\"-\" * 45)\n",
        "for word in comparison_words:\n",
        "    stemmed = porter.stem(word)\n",
        "    lemmatized = lemmatizer.lemmatize(word)\n",
        "    print(f\"{word:<15} {stemmed:<15} {lemmatized:<15}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Part-of-Speech (POS) Tagging\n",
        "\n",
        "POS tagging assigns grammatical categories (noun, verb, adjective, etc.) to each word in a sentence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using NLTK POS tagging\n",
        "text = \"Natural Language Processing is amazing and helps computers understand text.\"\n",
        "tokens = word_tokenize(text)\n",
        "pos_tags = pos_tag(tokens)\n",
        "\n",
        "print(\"NLTK POS Tagging:\")\n",
        "print(f\"{'Word':<20} {'POS Tag':<10}\")\n",
        "print(\"-\" * 30)\n",
        "for word, tag in pos_tags:\n",
        "    print(f\"{word:<20} {tag:<10}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using spaCy POS tagging\n",
        "doc = nlp(text)\n",
        "\n",
        "print(\"spaCy POS Tagging:\")\n",
        "print(f\"{'Word':<20} {'POS':<10} {'Tag':<10} {'Description':<30}\")\n",
        "print(\"-\" * 70)\n",
        "for token in doc:\n",
        "    if not token.is_punct and not token.is_space:\n",
        "        print(f\"{token.text:<20} {token.pos_:<10} {token.tag_:<10} {spacy.explain(token.pos_):<30}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. Named Entity Recognition (NER)\n",
        "\n",
        "NER identifies and classifies named entities in text (people, organizations, locations, dates, etc.).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using spaCy for NER\n",
        "doc = nlp(sample_text)\n",
        "\n",
        "print(\"spaCy Named Entity Recognition:\")\n",
        "print(f\"{'Entity':<25} {'Label':<15} {'Description':<30}\")\n",
        "print(\"-\" * 70)\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.text:<25} {ent.label_:<15} {spacy.explain(ent.label_):<30}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize NER with spaCy's displaCy (if available)\n",
        "# This creates an HTML visualization\n",
        "from spacy import displacy\n",
        "\n",
        "# Create a visualization of entities\n",
        "html = displacy.render(doc, style=\"ent\", jupyter=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract entities by type\n",
        "print(\"Entities by Type:\")\n",
        "entities_by_type = {}\n",
        "for ent in doc.ents:\n",
        "    if ent.label_ not in entities_by_type:\n",
        "        entities_by_type[ent.label_] = []\n",
        "    entities_by_type[ent.label_].append(ent.text)\n",
        "\n",
        "for label, entities in entities_by_type.items():\n",
        "    print(f\"\\n{label} ({spacy.explain(label)}):\")\n",
        "    print(f\"  {', '.join(set(entities))}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook covered:\n",
        "- ✅ **Tokenization**: Word and sentence tokenization using NLTK and spaCy\n",
        "- ✅ **Stemming**: Porter and Snowball stemmers for reducing words to root forms\n",
        "- ✅ **Lemmatization**: Converting words to dictionary forms with POS awareness\n",
        "- ✅ **POS Tagging**: Identifying grammatical roles of words\n",
        "- ✅ **NER**: Extracting named entities (people, organizations, locations, etc.)\n",
        "\n",
        "### When to Use What?\n",
        "\n",
        "- **Stemming**: Fast, good for search engines, information retrieval\n",
        "- **Lemmatization**: Better for tasks requiring valid words, more accurate but slower\n",
        "- **POS Tagging**: Needed for syntax analysis, dependency parsing, better lemmatization\n",
        "- **NER**: Extract structured information, build knowledge graphs, information extraction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
