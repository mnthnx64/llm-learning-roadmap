{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Text Representation\n",
        "\n",
        "This notebook covers different methods for representing text as numerical vectors that machine learning models can understand:\n",
        "- **Bag of Words (BOW)**: Simple word frequency representation\n",
        "- **Count Vectorizer**: Scikit-learn implementation of BOW\n",
        "- **TF-IDF (Term Frequency-Inverse Document Frequency)**: Weighted word importance\n",
        "- **Word2Vec**: Word embeddings using neural networks\n",
        "- **Modern Embeddings**: Overview of transformer-based embeddings\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "- Understand different text representation techniques\n",
        "- Convert text to numerical vectors using various methods\n",
        "- Compare and contrast different representation approaches\n",
        "- Apply embeddings for semantic similarity and clustering\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation\n",
        "\n",
        "Run this cell to install required packages (uncomment if needed):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install packages (uncomment if needed)\n",
        "# !pip install scikit-learn gensim numpy pandas matplotlib seaborn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set style for plots\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sample Documents\n",
        "\n",
        "We'll use these sample documents to demonstrate different text representation methods:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample documents\n",
        "documents = [\n",
        "    \"Machine learning is a subset of artificial intelligence\",\n",
        "    \"Artificial intelligence and machine learning are transforming technology\",\n",
        "    \"Deep learning is a type of machine learning\",\n",
        "    \"Natural language processing uses machine learning\",\n",
        "    \"Computer vision and image processing are important AI fields\"\n",
        "]\n",
        "\n",
        "print(\"Sample Documents:\")\n",
        "for i, doc in enumerate(documents, 1):\n",
        "    print(f\"{i}. {doc}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Bag of Words (BOW)\n",
        "\n",
        "Bag of Words is the simplest text representation method. It creates a vocabulary of all unique words and represents each document as a vector of word counts.\n",
        "\n",
        "## 1.1 Manual BOW Implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Manual BOW implementation\n",
        "def create_bow(documents):\n",
        "    \"\"\"Create a simple Bag of Words representation\"\"\"\n",
        "    # Build vocabulary\n",
        "    vocabulary = set()\n",
        "    for doc in documents:\n",
        "        words = doc.lower().split()\n",
        "        vocabulary.update(words)\n",
        "    \n",
        "    vocabulary = sorted(list(vocabulary))\n",
        "    \n",
        "    # Create BOW vectors\n",
        "    bow_vectors = []\n",
        "    for doc in documents:\n",
        "        words = doc.lower().split()\n",
        "        vector = [words.count(word) for word in vocabulary]\n",
        "        bow_vectors.append(vector)\n",
        "    \n",
        "    return vocabulary, np.array(bow_vectors)\n",
        "\n",
        "vocab, bow_matrix = create_bow(documents)\n",
        "\n",
        "print(\"Vocabulary:\", vocab)\n",
        "print(\"\\nBOW Matrix:\")\n",
        "print(pd.DataFrame(bow_matrix, columns=vocab, index=[f\"Doc {i+1}\" for i in range(len(documents))]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 Count Vectorizer (Scikit-learn)\n",
        "\n",
        "Scikit-learn's `CountVectorizer` provides a more robust BOW implementation with preprocessing options:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize CountVectorizer\n",
        "count_vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit and transform documents\n",
        "count_matrix = count_vectorizer.fit_transform(documents)\n",
        "\n",
        "# Get vocabulary\n",
        "vocabulary = count_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Convert to dense array for visualization\n",
        "count_array = count_matrix.toarray()\n",
        "\n",
        "print(\"Count Vectorizer Results:\")\n",
        "print(f\"Vocabulary size: {len(vocabulary)}\")\n",
        "print(\"\\nCount Matrix:\")\n",
        "df = pd.DataFrame(count_array, columns=vocabulary, index=[f\"Doc {i+1}\" for i in range(len(documents))])\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CountVectorizer with options\n",
        "count_vect = CountVectorizer(\n",
        "    lowercase=True,      # Convert to lowercase\n",
        "    stop_words='english', # Remove stop words\n",
        "    max_features=50,      # Limit vocabulary size\n",
        "    ngram_range=(1, 2)   # Use unigrams and bigrams\n",
        ")\n",
        "\n",
        "count_matrix_advanced = count_vect.fit_transform(documents)\n",
        "vocab_advanced = count_vect.get_feature_names_out()\n",
        "\n",
        "print(\"Advanced Count Vectorizer (with stop words removal and bigrams):\")\n",
        "print(f\"Vocabulary size: {len(vocab_advanced)}\")\n",
        "print(f\"Sample vocabulary: {list(vocab_advanced[:10])}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. TF-IDF (Term Frequency-Inverse Document Frequency)\n",
        "\n",
        "TF-IDF weights words by their importance: frequent in a document but rare across all documents get higher weights.\n",
        "\n",
        "**TF (Term Frequency)**: How often a term appears in a document\n",
        "**IDF (Inverse Document Frequency)**: How rare a term is across all documents\n",
        "\n",
        "$$TF\\text{-}IDF(t, d) = TF(t, d) \\times IDF(t)$$\n",
        "\n",
        "$$IDF(t) = \\log\\frac{N}{df(t)}$$\n",
        "\n",
        "where N is the total number of documents and df(t) is the number of documents containing term t.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform documents\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "\n",
        "# Get vocabulary\n",
        "tfidf_vocab = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Convert to dense array for visualization\n",
        "tfidf_array = tfidf_matrix.toarray()\n",
        "\n",
        "print(\"TF-IDF Results:\")\n",
        "print(f\"Vocabulary size: {len(tfidf_vocab)}\")\n",
        "print(\"\\nTF-IDF Matrix:\")\n",
        "df_tfidf = pd.DataFrame(tfidf_array, columns=tfidf_vocab, index=[f\"Doc {i+1}\" for i in range(len(documents))])\n",
        "print(df_tfidf.round(3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show top terms for each document\n",
        "print(\"Top TF-IDF Terms per Document:\")\n",
        "for i, doc in enumerate(documents):\n",
        "    # Get TF-IDF scores for this document\n",
        "    scores = tfidf_array[i]\n",
        "    # Get indices of top 5 terms\n",
        "    top_indices = scores.argsort()[-5:][::-1]\n",
        "    top_terms = [(tfidf_vocab[idx], scores[idx]) for idx in top_indices]\n",
        "    \n",
        "    print(f\"\\nDoc {i+1}: {doc[:50]}...\")\n",
        "    for term, score in top_terms:\n",
        "        print(f\"  {term}: {score:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate document similarity using TF-IDF\n",
        "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "print(\"Document Similarity Matrix (Cosine Similarity):\")\n",
        "df_sim = pd.DataFrame(similarity_matrix, \n",
        "                     index=[f\"Doc {i+1}\" for i in range(len(documents))],\n",
        "                     columns=[f\"Doc {i+1}\" for i in range(len(documents))])\n",
        "print(df_sim.round(3))\n",
        "\n",
        "# Visualize similarity matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(similarity_matrix, annot=True, fmt='.2f', cmap='YlOrRd', \n",
        "            xticklabels=[f\"Doc {i+1}\" for i in range(len(documents))],\n",
        "            yticklabels=[f\"Doc {i+1}\" for i in range(len(documents))])\n",
        "plt.title('Document Similarity (TF-IDF Cosine Similarity)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Word2Vec\n",
        "\n",
        "Word2Vec creates dense vector representations (embeddings) where similar words have similar vectors. It captures semantic relationships.\n",
        "\n",
        "Two architectures:\n",
        "- **CBOW (Continuous Bag of Words)**: Predicts word from context\n",
        "- **Skip-gram**: Predicts context from word\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare text data for Word2Vec (tokenized sentences)\n",
        "tokenized_docs = [doc.lower().split() for doc in documents]\n",
        "\n",
        "print(\"Tokenized Documents:\")\n",
        "for i, tokens in enumerate(tokenized_docs, 1):\n",
        "    print(f\"Doc {i}: {tokens}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Word2Vec model\n",
        "# Parameters:\n",
        "# - size: dimension of word vectors (embeddings)\n",
        "# - window: maximum distance between current and predicted word\n",
        "# - min_count: ignore words with frequency < min_count\n",
        "# - sg: 0 for CBOW, 1 for Skip-gram\n",
        "\n",
        "word2vec_model = Word2Vec(\n",
        "    sentences=tokenized_docs,\n",
        "    vector_size=100,      # Dimension of word vectors\n",
        "    window=5,             # Context window size\n",
        "    min_count=1,          # Minimum word frequency\n",
        "    workers=4,            # Number of threads\n",
        "    sg=0                  # 0 = CBOW, 1 = Skip-gram\n",
        ")\n",
        "\n",
        "print(\"Word2Vec model trained!\")\n",
        "print(f\"Vocabulary size: {len(word2vec_model.wv.key_to_index)}\")\n",
        "print(f\"Vector dimension: {word2vec_model.wv.vector_size}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get word vectors\n",
        "words_to_check = ['machine', 'learning', 'artificial', 'intelligence']\n",
        "\n",
        "print(\"Word Vectors (first 10 dimensions):\")\n",
        "for word in words_to_check:\n",
        "    if word in word2vec_model.wv:\n",
        "        vector = word2vec_model.wv[word]\n",
        "        print(f\"\\n{word}: {vector[:10]}... (shape: {vector.shape})\")\n",
        "    else:\n",
        "        print(f\"\\n{word}: Not in vocabulary\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find most similar words\n",
        "test_words = ['machine', 'learning', 'artificial']\n",
        "\n",
        "print(\"Most Similar Words:\")\n",
        "for word in test_words:\n",
        "    if word in word2vec_model.wv:\n",
        "        similar = word2vec_model.wv.most_similar(word, topn=5)\n",
        "        print(f\"\\nWords similar to '{word}':\")\n",
        "        for similar_word, score in similar:\n",
        "            print(f\"  {similar_word}: {score:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Word analogies (e.g., king - man + woman â‰ˆ queen)\n",
        "# This demonstrates semantic relationships captured by Word2Vec\n",
        "\n",
        "print(\"Word Analogies:\")\n",
        "print(\"Note: With limited training data, results may not be perfect\")\n",
        "\n",
        "# Example: learning - machine + artificial â‰ˆ ?\n",
        "if all(word in word2vec_model.wv for word in ['learning', 'machine', 'artificial']):\n",
        "    result = word2vec_model.wv.most_similar(\n",
        "        positive=['learning', 'artificial'],\n",
        "        negative=['machine'],\n",
        "        topn=3\n",
        "    )\n",
        "    print(f\"\\n'learning' - 'machine' + 'artificial' â‰ˆ ?\")\n",
        "    for word, score in result:\n",
        "        print(f\"  {word}: {score:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get document embeddings by averaging word vectors\n",
        "def get_document_embedding(doc_tokens, model):\n",
        "    \"\"\"Get document embedding by averaging word vectors\"\"\"\n",
        "    words = [word for word in doc_tokens if word in model.wv]\n",
        "    if len(words) == 0:\n",
        "        return None\n",
        "    vectors = [model.wv[word] for word in words]\n",
        "    return np.mean(vectors, axis=0)\n",
        "\n",
        "# Get embeddings for all documents\n",
        "doc_embeddings = []\n",
        "for i, tokens in enumerate(tokenized_docs):\n",
        "    embedding = get_document_embedding(tokens, word2vec_model)\n",
        "    if embedding is not None:\n",
        "        doc_embeddings.append(embedding)\n",
        "        print(f\"Doc {i+1} embedding shape: {embedding.shape}\")\n",
        "    else:\n",
        "        print(f\"Doc {i+1}: No valid words found\")\n",
        "\n",
        "doc_embeddings = np.array(doc_embeddings)\n",
        "print(f\"\\nDocument embeddings matrix shape: {doc_embeddings.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Modern Embeddings Overview\n",
        "\n",
        "Modern NLP uses transformer-based models that generate contextual embeddings. Here's an overview:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.1 Types of Modern Embeddings\n",
        "\n",
        "### Contextual vs Non-Contextual\n",
        "\n",
        "- **Non-Contextual (Word2Vec, GloVe)**: Same word always has the same embedding\n",
        "  - Example: \"bank\" (river bank) and \"bank\" (financial bank) have the same vector\n",
        "  \n",
        "- **Contextual (BERT, GPT, etc.)**: Same word has different embeddings based on context\n",
        "  - Example: \"bank\" in \"river bank\" vs \"bank account\" have different embeddings\n",
        "\n",
        "### Popular Embedding Models\n",
        "\n",
        "1. **Word2Vec** (2013): Word-level embeddings, non-contextual\n",
        "2. **GloVe** (2014): Global word co-occurrence statistics\n",
        "3. **ELMo** (2018): Contextual embeddings using bidirectional LSTMs\n",
        "4. **BERT** (2018): Bidirectional transformer, contextual embeddings\n",
        "5. **GPT** (2018+): Generative transformer models\n",
        "6. **Sentence Transformers** (2019+): Optimized for sentence-level embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Using sentence-transformers (install with: pip install sentence-transformers)\n",
        "# This is commented out but shows how to use modern embeddings\n",
        "\n",
        "\"\"\"\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load a pre-trained model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Generate embeddings\n",
        "embeddings = model.encode(documents)\n",
        "\n",
        "print(f\"Embedding shape: {embeddings.shape}\")\n",
        "print(f\"Each document is represented as a {embeddings.shape[1]}-dimensional vector\")\n",
        "\n",
        "# Calculate similarity\n",
        "similarity = cosine_similarity(embeddings)\n",
        "print(\"\\nDocument Similarity (using Sentence Transformers):\")\n",
        "print(pd.DataFrame(similarity, \n",
        "                   index=[f\"Doc {i+1}\" for i in range(len(documents))],\n",
        "                   columns=[f\"Doc {i+1}\" for i in range(len(documents))]).round(3))\n",
        "\"\"\"\n",
        "print(\"Note: Install sentence-transformers to use modern embeddings\")\n",
        "print(\"pip install sentence-transformers\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. Comparison and When to Use What\n",
        "\n",
        "## Comparison Table\n",
        "\n",
        "| Method | Dimensions | Contextual | Speed | Use Case |\n",
        "|--------|-----------|------------|-------|----------|\n",
        "| **BOW/Count** | Vocabulary size | âŒ No | âš¡ Very Fast | Simple classification, baseline |\n",
        "| **TF-IDF** | Vocabulary size | âŒ No | âš¡ Fast | Information retrieval, search |\n",
        "| **Word2Vec** | 100-300 | âŒ No | âš¡ Fast | Word similarity, analogies |\n",
        "| **BERT/Transformers** | 768-1024+ | âœ… Yes | ðŸŒ Slow | Semantic understanding, modern NLP |\n",
        "\n",
        "## Recommendations\n",
        "\n",
        "- **BOW/TF-IDF**: \n",
        "  - Good for: Simple classification, baseline models, small datasets\n",
        "  - Bad for: Semantic understanding, capturing word relationships\n",
        "  \n",
        "- **Word2Vec**: \n",
        "  - Good for: Word similarity, analogies, when you need word-level embeddings\n",
        "  - Bad for: Context-dependent meanings, capturing sentence semantics\n",
        "  \n",
        "- **Modern Embeddings (BERT, etc.)**: \n",
        "  - Good for: Semantic search, understanding context, state-of-the-art results\n",
        "  - Bad for: Real-time applications (unless optimized), very large datasets without GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the difference in dimensionality\n",
        "methods = ['BOW', 'TF-IDF', 'Word2Vec', 'BERT']\n",
        "vocab_size = len(vocabulary)\n",
        "dimensions = [vocab_size, vocab_size, 100, 768]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "bars = ax.bar(methods, dimensions, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])\n",
        "ax.set_ylabel('Vector Dimension', fontsize=12)\n",
        "ax.set_title('Comparison of Text Representation Dimensions', fontsize=14, fontweight='bold')\n",
        "ax.set_yscale('log')  # Log scale to better show differences\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{int(height)}',\n",
        "            ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Note: BOW/TF-IDF dimensions = vocabulary size ({vocab_size} words)\")\n",
        "print(\"Word2Vec typically uses 100-300 dimensions\")\n",
        "print(\"BERT uses 768-1024 dimensions\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook covered:\n",
        "- âœ… **Bag of Words (BOW)**: Simple word frequency representation\n",
        "- âœ… **Count Vectorizer**: Scikit-learn implementation with preprocessing options\n",
        "- âœ… **TF-IDF**: Weighted word importance for better document representation\n",
        "- âœ… **Word2Vec**: Neural network-based word embeddings\n",
        "- âœ… **Modern Embeddings**: Overview of transformer-based contextual embeddings\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **BOW/TF-IDF** are sparse, high-dimensional representations suitable for traditional ML\n",
        "2. **Word2Vec** creates dense, low-dimensional embeddings capturing word relationships\n",
        "3. **Modern embeddings** (BERT, etc.) provide contextual understanding but are computationally expensive\n",
        "4. Choose the right representation based on your task, data size, and computational resources\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
